{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2nd_Model",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCO4kZE0ztpv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<h2>Target</h2>\n",
        "<ul>\n",
        "<li>Generative (not retrieval)</li>\n",
        "<li>Small-talk (short convos)</li>\n",
        "<li>Open Domain</li>\n",
        "<li>No context</li>\n",
        "<li>Eval [https://arxiv.org/abs/1603.08023]</li>\n",
        "</ul>\n",
        "<h2>Potential Sources of Data</h2>\n",
        "<ul>\n",
        "<li>http://files.pushshift.io/reddit/comments/</li>\n",
        "<li>https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html</li>\n",
        "<li>https://github.com/Marsan-Ma/twitter_scraper</li>\n",
        "<li>http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/</li>\n",
        "</ul>\n",
        "<h2>Potential Models</h2>\n",
        "<h3>This Medium Article</h3>\n",
        "<ul>\n",
        "<li>https://medium.com/botsupply/generative-model-chatbots-e422ab08461e</li>\n",
        "<li>Use high LR, then reduce</li>\n",
        "<li>Reverse input sentence</li>\n",
        "<li>50 K vocab</li>\n",
        "</ul>\n",
        "<h3>LSTM Encoder Decoder</h3>\n",
        "<h2></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHc7Oe-WR7VH",
        "colab_type": "code",
        "outputId": "a5517ffb-ff3c-4296-9794-8b0e43d9e5d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%cd ..\n",
        "%cd root\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"dwdata_2.csv\")\n",
        "df.fillna('', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-cJELvuSQ87",
        "colab_type": "code",
        "outputId": "0347a5d9-e376-481b-abfc-6b618889514b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_1 = df[\"X_1\"].tolist()\n",
        "y_1 = df[\"Y_1\"].tolist()\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "# from tensorflow_datasets.features.text.SubwordTextEncoder import build_from_corpus\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "token_x = tfds.features.text.SubwordTextEncoder.build_from_corpus(x_1, target_vocab_size=1e12)\n",
        "token_y = tfds.features.text.SubwordTextEncoder.build_from_corpus(y_1, target_vocab_size=1e12)\n",
        "\n",
        "vocab_size_x = token_x.vocab_size\n",
        "vocab_size_y = token_y.vocab_size + 2 # adding the start and end characters\n",
        "\n",
        "inputs = [token_x.encode(sentence) for sentence in x_1]\n",
        "outputs = [[vocab_size_y - 2] + token_y.encode(sentence) + [vocab_size_y - 1] for sentence in y_1]\n",
        "\n",
        "MAXLEN = 70\n",
        "idx_to_shorten = [count for count, sent in enumerate(inputs) if len(sent) > MAXLEN]\n",
        "for idx in idx_to_shorten:\n",
        "    inputs[idx] = inputs[idx][:MAXLEN]\n",
        "    outputs[idx] = outputs[idx][:MAXLEN]\n",
        "idx_to_shorten = []\n",
        "idx_to_shorten = [count for count, sent in enumerate(outputs) if len(sent) > MAXLEN]\n",
        "for idx in idx_to_shorten:\n",
        "    inputs[idx] = inputs[idx][:MAXLEN]\n",
        "    outputs[idx] = outputs[idx][:MAXLEN]\n",
        "\n",
        "MINLEN = 4\n",
        "idx_to_delete = [count for count, sent in enumerate(inputs) if len(sent) < MINLEN]\n",
        "for idx in reversed(idx_to_delete):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_delete = []\n",
        "idx_to_delete = [count for count, sent in enumerate(outputs) if len(sent) < MINLEN]\n",
        "for idx in reversed(idx_to_delete):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "inputs = pad_sequences(inputs, value=0, padding=\"post\", maxlen=MAXLEN)\n",
        "outputs = pad_sequences(outputs, value=0, padding=\"post\", maxlen=MAXLEN)\n",
        "print(\"{},{}\".format(inputs.shape,outputs.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9413, 70),(9413, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzN0BhHIslF8",
        "colab_type": "code",
        "outputId": "a71ff503-3f86-4464-840c-4f159eca5d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "token_x.decode(np.squeeze(inputs_1[0],axis=-1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'im pleased you two are so impressed i find the whole placehateful'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgTzWkZ5YSNf",
        "colab_type": "code",
        "outputId": "60d93500-2dea-45e5-b630-9a51bd5f366d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage.interpolation import shift\n",
        "inputs_1 = inputs\n",
        "inputs_2 = outputs\n",
        "outputs_1 = outputs\n",
        "outputs_1 = shift(outputs_1, [0,-1], cval=0)\n",
        "print(\"{},{}\".format(inputs_1.shape,outputs_1.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(55455, 70),(55455, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_nyvc1mgeQw",
        "colab_type": "code",
        "outputId": "c01e52de-1133-43d5-f79e-ce039a475079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "print(inputs_1[20,:20])\n",
        "print(inputs_2[20,:20])\n",
        "print(outputs_1[20,:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  22    7   24  716   92 3330   17   40  381    3   34  273   50    0\n",
            "    0    0    0    0    0    0]\n",
            "[44307    22    14   726    28     4 18915   267  1505    32     4   407\n",
            "    72     5  1346   646     3    89     5   371]\n",
            "[   22    14   726    28     4 18915   267  1505    32     4   407    72\n",
            "     5  1346   646     3    89     5   371  2078]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRDEL7Uxh4bY",
        "colab_type": "code",
        "outputId": "83252ea5-32ba-4420-eab6-80fde88f5c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "inputs_1 = np.expand_dims(inputs_1,axis=-1)\n",
        "inputs_2 = np.expand_dims(inputs_2,axis=-1)\n",
        "outputs_1 = np.expand_dims(outputs_1,axis=-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6856365723f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minputs_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutputs_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryL02M2_OT6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 128\n",
        "batch_size = 32\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC2oxeFbimAl",
        "colab_type": "code",
        "outputId": "fb36b6d0-be1d-4af4-fe56-ec01e016fe45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"{},{}\".format(vocab_size_x,vocab_size_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37090,44309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW40jyTJIdKK",
        "colab_type": "code",
        "outputId": "fbeb0d8e-3cfc-4d4e-8526-d9733bfe5252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#We do the most basic one\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "encoder_inputs = Input(shape=(None,1))\n",
        "encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,1))\n",
        "decoder_outputs, _, _ = LSTM(latent_dim, return_sequences=True, return_state=True)(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "hist = model.fit([inputs_1, inputs_2], outputs_1,batch_size=batch_size,epochs=epochs,validation_split=0.2)\n",
        "model.save(\"2nd_m.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1387/1387 [==============================] - 17s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 2/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 3/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 4/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 5/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 6/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 7/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 8/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 9/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 10/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 11/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 12/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 13/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 14/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 15/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 16/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 17/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 18/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 19/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 20/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 21/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 22/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 23/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 24/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 25/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 26/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 27/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 28/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 29/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 30/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 31/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 32/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 33/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 34/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 35/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 36/100\n",
            "1387/1387 [==============================] - 17s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 37/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 38/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 39/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 40/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 41/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 42/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 43/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 44/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 45/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 46/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 47/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 48/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 49/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 50/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 51/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 52/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 53/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 54/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 55/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 56/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 57/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 58/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 59/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 60/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 61/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 62/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 63/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 64/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 65/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 66/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 67/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 68/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 69/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 70/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 71/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 72/100\n",
            "1387/1387 [==============================] - 17s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 73/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 74/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 75/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 76/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 77/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 78/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 79/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 80/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1647e-04 - val_loss: 1.1677e-04\n",
            "Epoch 81/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 82/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 83/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 84/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 85/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 86/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 87/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 88/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 89/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 90/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 91/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 92/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 93/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 94/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 95/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 96/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 97/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 98/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 99/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 100/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7jkorZCoSBk",
        "colab_type": "text"
      },
      "source": [
        "The model might have a case of vanishing gradient problem, and it's underfitting (plateauing loss) --> increase cells / increase layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKdAkEgho-V5",
        "colab_type": "code",
        "outputId": "c5b3d729-5280-4308-a4f1-7681dab12bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist.history[\"loss\"], color=\"r\")\n",
        "plt.plot(hist.history[\"val_loss\"], color=\"b\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4e125903c8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkPphhKNjyW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = LSTM(latent_dim, return_sequences=True, return_state=True)(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSlDzQn_oEI6",
        "colab_type": "code",
        "outputId": "91b7be27-48c2-4f14-fbf7-30c6857285fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    input_seq = np.expand_dims(input_seq,axis=-1)\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1,1,1))\n",
        "    target_seq[0, 0, 0] = vocab_size_y - 2\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = []\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = output_tokens[:,-1,:]\n",
        "        decoded_sentence.append(sampled_token_index[0][0])\n",
        "\n",
        "        if (sampled_token_index == vocab_size_y - 1 or len(decoded_sentence) > MAXLEN):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1,1,1))\n",
        "        target_seq[0,0,0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "    pred = token_y.decode([int(i) for i in decoded_sentence])\n",
        "    return pred\n",
        "index_try = 33\n",
        "input_seq = inputs_1[index_try: index_try + 1]\n",
        "pred = decode_sequence(input_seq)\n",
        "print('-')\n",
        "print('Input sentence:', token_x.decode(np.squeeze(inputs_1[index_try], axis=-1)))\n",
        "print('Decoded sentence:', pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: ill just fetch my galoshes\n",
            "Decoded sentence: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNXkd2ozIaG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnp-6f355f3e5639\n",
        "encoder_inputs = Input(shape=(maxlen, ), dtype='int32',)\n",
        "encoder_embedding = embed_layer(encoder_inputs)\n",
        "encoder_LSTM = LSTM(128, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
        "\n",
        "decoder_inputs = Input(shape=(maxlen, ), dtype='int32',)\n",
        "decoder_embedding = embed_layer(decoder_inputs)\n",
        "decoder_LSTM = LSTM(128, return_state=True, return_sequences=True)\n",
        "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "outputs = TimeDistributed(Dense(20000, activation='softmax'))(decoder_outputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "model.summary()\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}