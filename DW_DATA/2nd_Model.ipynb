{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2nd_Model",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCO4kZE0ztpv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<h2>Target</h2>\n",
        "<ul>\n",
        "<li>Generative (not retrieval)</li>\n",
        "<li>Small-talk (short convos)</li>\n",
        "<li>Open Domain</li>\n",
        "<li>No context</li>\n",
        "<li>Eval [https://arxiv.org/abs/1603.08023]</li>\n",
        "</ul>\n",
        "<h2>Potential Sources of Data</h2>\n",
        "<ul>\n",
        "<li>http://files.pushshift.io/reddit/comments/</li>\n",
        "<li>https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html</li>\n",
        "<li>https://github.com/Marsan-Ma/twitter_scraper</li>\n",
        "<li>http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/</li>\n",
        "</ul>\n",
        "<h2>Potential Models</h2>\n",
        "<h3>This Medium Article</h3>\n",
        "<ul>\n",
        "<li>https://medium.com/botsupply/generative-model-chatbots-e422ab08461e</li>\n",
        "<li>Use high LR, then reduce</li>\n",
        "<li>Reverse input sentence</li>\n",
        "<li>50 K vocab</li>\n",
        "</ul>\n",
        "<h3>LSTM Encoder Decoder</h3>\n",
        "<h2></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHc7Oe-WR7VH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3e18186e-f660-4f8e-b6cc-edd81099ebf6"
      },
      "source": [
        "%cd ..\n",
        "%cd root\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"dwdata_2.csv\")\n",
        "df.fillna('', inplace=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-cJELvuSQ87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c449c2ba-6285-4694-cc7c-567ca2b407e9"
      },
      "source": [
        "x_1 = df[\"X_1\"].tolist()\n",
        "y_1 = df[\"Y_1\"].tolist()\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "# from tensorflow_datasets.features.text.SubwordTextEncoder import build_from_corpus\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "token_x = tfds.features.text.SubwordTextEncoder.build_from_corpus(x_1, target_vocab_size=1e12)\n",
        "token_y = tfds.features.text.SubwordTextEncoder.build_from_corpus(y_1, target_vocab_size=1e12)\n",
        "\n",
        "vocab_size_x = token_x.vocab_size\n",
        "vocab_size_y = token_y.vocab_size + 2 # adding the start and end characters\n",
        "\n",
        "inputs = [token_x.encode(sentence) for sentence in x_1]\n",
        "outputs = [[vocab_size_y - 2] + token_y.encode(sentence) + [vocab_size_y - 1] for sentence in y_1]\n",
        "\n",
        "MAXLEN = 70\n",
        "idx_to_shorten = [count for count, sent in enumerate(inputs) if len(sent) > MAXLEN]\n",
        "for idx in idx_to_shorten:\n",
        "    inputs[idx] = inputs[idx][:MAXLEN]\n",
        "    outputs[idx] = outputs[idx][:MAXLEN]\n",
        "idx_to_shorten = []\n",
        "idx_to_shorten = [count for count, sent in enumerate(outputs) if len(sent) > MAXLEN]\n",
        "for idx in idx_to_shorten:\n",
        "    inputs[idx] = inputs[idx][:MAXLEN]\n",
        "    outputs[idx] = outputs[idx][:MAXLEN]\n",
        "\n",
        "MINLEN = 4\n",
        "idx_to_delete = [count for count, sent in enumerate(inputs) if len(sent) < MINLEN]\n",
        "for idx in reversed(idx_to_delete):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_delete = []\n",
        "idx_to_delete = [count for count, sent in enumerate(outputs) if len(sent) < MINLEN]\n",
        "for idx in reversed(idx_to_delete):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "inputs = pad_sequences(inputs, value=0, padding=\"post\", maxlen=MAXLEN)\n",
        "outputs = pad_sequences(outputs, value=0, padding=\"post\", maxlen=MAXLEN)\n",
        "print(\"{},{}\".format(inputs.shape,outputs.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(55455, 70),(55455, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgTzWkZ5YSNf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "60d93500-2dea-45e5-b630-9a51bd5f366d"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage.interpolation import shift\n",
        "inputs_1 = inputs\n",
        "inputs_2 = outputs\n",
        "outputs_1 = outputs\n",
        "outputs_1 = shift(outputs_1, [0,-1], cval=0)\n",
        "print(\"{},{}\".format(inputs_1.shape,outputs_1.shape))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(55455, 70),(55455, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_nyvc1mgeQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "c01e52de-1133-43d5-f79e-ce039a475079"
      },
      "source": [
        "print(inputs_1[20,:20])\n",
        "print(inputs_2[20,:20])\n",
        "print(outputs_1[20,:20])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  22    7   24  716   92 3330   17   40  381    3   34  273   50    0\n",
            "    0    0    0    0    0    0]\n",
            "[44307    22    14   726    28     4 18915   267  1505    32     4   407\n",
            "    72     5  1346   646     3    89     5   371]\n",
            "[   22    14   726    28     4 18915   267  1505    32     4   407    72\n",
            "     5  1346   646     3    89     5   371  2078]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRDEL7Uxh4bY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs_1 = np.expand_dims(inputs_1,axis=-1)\n",
        "inputs_2 = np.expand_dims(inputs_2,axis=-1)\n",
        "outputs_1 = np.expand_dims(outputs_1,axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryL02M2_OT6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 128\n",
        "batch_size = 32\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC2oxeFbimAl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb36b6d0-be1d-4af4-fe56-ec01e016fe45"
      },
      "source": [
        "print(\"{},{}\".format(vocab_size_x,vocab_size_y))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37090,44309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW40jyTJIdKK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "fbeb0d8e-3cfc-4d4e-8526-d9733bfe5252"
      },
      "source": [
        "#We do the most basic one\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "encoder_inputs = Input(shape=(MAXLEN,1))\n",
        "encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(MAXLEN,1))\n",
        "decoder_outputs, _, _ = LSTM(latent_dim, return_sequences=True, return_state=True)(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "hist = model.fit([inputs_1, inputs_2], outputs_1,batch_size=batch_size,epochs=epochs,validation_split=0.2)\n",
        "model.save(\"2nd_m.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1387/1387 [==============================] - 17s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 2/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 3/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1646e-04 - val_loss: 1.1677e-04\n",
            "Epoch 4/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1644e-04 - val_loss: 1.1677e-04\n",
            "Epoch 5/100\n",
            "1387/1387 [==============================] - 16s 12ms/step - loss: 1.1645e-04 - val_loss: 1.1677e-04\n",
            "Epoch 6/100\n",
            "  77/1387 [>.............................] - ETA: 13s - loss: 1.1706e-04"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNXkd2ozIaG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnp-6f355f3e5639\n",
        "encoder_inputs = Input(shape=(maxlen, ), dtype='int32',)\n",
        "encoder_embedding = embed_layer(encoder_inputs)\n",
        "encoder_LSTM = LSTM(128, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
        "\n",
        "decoder_inputs = Input(shape=(maxlen, ), dtype='int32',)\n",
        "decoder_embedding = embed_layer(decoder_inputs)\n",
        "decoder_LSTM = LSTM(128, return_state=True, return_sequences=True)\n",
        "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "outputs = TimeDistributed(Dense(20000, activation='softmax'))(decoder_outputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "model.summary()\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}