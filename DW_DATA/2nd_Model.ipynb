{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2nd_Model",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCO4kZE0ztpv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<h2>Target</h2>\n",
        "<ul>\n",
        "<li>Generative (not retrieval)</li>\n",
        "<li>Small-talk (short convos)</li>\n",
        "<li>Open Domain</li>\n",
        "<li>No context</li>\n",
        "<li>Eval [https://arxiv.org/abs/1603.08023]</li>\n",
        "</ul>\n",
        "<h2>Potential Sources of Data</h2>\n",
        "<ul>\n",
        "<li>http://files.pushshift.io/reddit/comments/</li>\n",
        "<li>https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html</li>\n",
        "<li>https://github.com/Marsan-Ma/twitter_scraper</li>\n",
        "<li>http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/</li>\n",
        "</ul>\n",
        "<h2>Potential Models</h2>\n",
        "<h3>This Medium Article</h3>\n",
        "<ul>\n",
        "<li>https://medium.com/botsupply/generative-model-chatbots-e422ab08461e</li>\n",
        "<li>Use high LR, then reduce</li>\n",
        "<li>Reverse input sentence</li>\n",
        "<li>50 K vocab</li>\n",
        "</ul>\n",
        "<h3>LSTM Encoder Decoder</h3>\n",
        "<h2></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHc7Oe-WR7VH",
        "colab_type": "code",
        "outputId": "064f914a-c871-4ec3-d83c-fc71372b20e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%cd ..\n",
        "%cd root\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"dwdata_2.csv\")\n",
        "df.fillna('', inplace=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-cJELvuSQ87",
        "colab_type": "code",
        "outputId": "7c8493d1-e2d3-45a2-d6b8-ab0bd8a8c66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_1 = df[\"X_1\"].tolist()\n",
        "y_1 = df[\"Y_1\"].tolist()\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "# from tensorflow_datasets.features.text.SubwordTextEncoder import build_from_corpus\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "token_x = tfds.features.text.SubwordTextEncoder.build_from_corpus(x_1, target_vocab_size=1e12)\n",
        "token_y = tfds.features.text.SubwordTextEncoder.build_from_corpus(y_1, target_vocab_size=1e12)\n",
        "\n",
        "vocab_size_x = token_x.vocab_size\n",
        "vocab_size_y = token_y.vocab_size + 2 # adding the start and end characters\n",
        "\n",
        "inputs = [token_x.encode(sentence) for sentence in x_1]\n",
        "outputs = [[vocab_size_y - 2] + token_y.encode(sentence) + [vocab_size_y - 1] for sentence in y_1]\n",
        "\n",
        "MAXLEN = 70\n",
        "idx_to_shorten = [count for count, sent in enumerate(inputs) if len(sent) > MAXLEN]\n",
        "for idx in idx_to_shorten:\n",
        "    inputs[idx] = inputs[idx][:MAXLEN]\n",
        "    outputs[idx] = outputs[idx][:MAXLEN]\n",
        "idx_to_shorten = []\n",
        "idx_to_shorten = [count for count, sent in enumerate(outputs) if len(sent) > MAXLEN]\n",
        "for idx in idx_to_shorten:\n",
        "    inputs[idx] = inputs[idx][:MAXLEN]\n",
        "    outputs[idx] = outputs[idx][:MAXLEN]\n",
        "\n",
        "MINLEN = 4\n",
        "idx_to_delete = [count for count, sent in enumerate(inputs) if len(sent) < MINLEN]\n",
        "for idx in reversed(idx_to_delete):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_delete = []\n",
        "idx_to_delete = [count for count, sent in enumerate(outputs) if len(sent) < MINLEN]\n",
        "for idx in reversed(idx_to_delete):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "inputs = pad_sequences(inputs, value=0, padding=\"post\", maxlen=MAXLEN)\n",
        "outputs = pad_sequences(outputs, value=0, padding=\"post\", maxlen=MAXLEN)\n",
        "print(\"{},{}\".format(inputs.shape,outputs.shape))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21771, 70),(21771, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgTzWkZ5YSNf",
        "colab_type": "code",
        "outputId": "df489cc0-c7c6-43aa-f013-d8adef57c5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage.interpolation import shift\n",
        "inputs_1 = inputs\n",
        "inputs_2 = outputs\n",
        "outputs_1 = outputs\n",
        "outputs_1 = shift(outputs_1, [0,-1], cval=0)\n",
        "print(\"{},{}\".format(inputs_1.shape,outputs_1.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21771, 70),(21771, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_nyvc1mgeQw",
        "colab_type": "code",
        "outputId": "664b0c9c-95e9-409a-ab3b-d26515c43fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "print(inputs_1[20,:20])\n",
        "print(inputs_2[20,:20])\n",
        "print(outputs_1[20,:20])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  22    7   23  672   95 3744   16   41  418    3   35  274   49    0\n",
            "    0    0    0    0    0    0]\n",
            "[27074    22    14   809    28     4 10094   255  1314    31     4   423\n",
            "    67     5  1448   608     3    90     5   365]\n",
            "[   22    14   809    28     4 10094   255  1314    31     4   423    67\n",
            "     5  1448   608     3    90     5   365  2681]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRDEL7Uxh4bY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs_1 = np.expand_dims(inputs_1,axis=-1)\n",
        "inputs_2 = np.expand_dims(inputs_2,axis=-1)\n",
        "outputs_1 = np.expand_dims(outputs_1,axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzN0BhHIslF8",
        "colab_type": "code",
        "outputId": "cf4d4dfe-ed74-47fb-d4a2-91a08803a1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "print(token_x.decode(np.squeeze(inputs_1[0],axis=-1)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "im pleased you two are so impressed i find the whole placehateful\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryL02M2_OT6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 256\n",
        "batch_size = 32\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC2oxeFbimAl",
        "colab_type": "code",
        "outputId": "8b2a7041-5cff-44e3-adcf-dacb0ab34381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"{},{}\".format(vocab_size_x,vocab_size_y))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22801,27076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW40jyTJIdKK",
        "colab_type": "code",
        "outputId": "1e4351a5-e14d-476d-92db-ed8e1489a024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#We do the most basic one\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "encoder_inputs = Input(shape=(None,1))\n",
        "encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,1))\n",
        "decoder_outputs, _, _ = LSTM(latent_dim, return_sequences=True, return_state=True)(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "adam = Adam(lr=0.1, decay=1e-6)\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "hist = model.fit([inputs_1, inputs_2], outputs_1,batch_size=batch_size,epochs=epochs,validation_split=0.2)\n",
        "model.save(\"2nd_m.h5\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "545/545 [==============================] - 10s 18ms/step - loss: 7.9333e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 2/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9330e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 3/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9322e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 4/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9328e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 5/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9351e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 6/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9338e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 7/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9339e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 8/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9370e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 9/100\n",
            "545/545 [==============================] - 9s 17ms/step - loss: 7.9327e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 10/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9378e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 11/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9338e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 12/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9347e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 13/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9360e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 14/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9344e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 15/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9337e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 16/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9354e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 17/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9353e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 18/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9405e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 19/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9341e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 20/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9356e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 21/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9342e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 22/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9346e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 23/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9333e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 24/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9349e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 25/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9330e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 26/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9325e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 27/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9400e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 28/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9343e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 29/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9321e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 30/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9327e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 31/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9334e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 32/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9332e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 33/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9355e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 34/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9361e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 35/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9376e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 36/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9378e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 37/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9336e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 38/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9342e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 39/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9342e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 40/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9337e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 41/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9309e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 42/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9363e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 43/100\n",
            "545/545 [==============================] - 9s 17ms/step - loss: 7.9363e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 44/100\n",
            "545/545 [==============================] - 9s 17ms/step - loss: 7.9351e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 45/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9404e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 46/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9372e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 47/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9361e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 48/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9349e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 49/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9347e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 50/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9340e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 51/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9329e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 52/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9319e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 53/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9337e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 54/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9381e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 55/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9328e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 56/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9344e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 57/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9323e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 58/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9334e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 59/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9348e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 60/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9335e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 61/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9339e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 62/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9339e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 63/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9360e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 64/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9357e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 65/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9334e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 66/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9339e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 67/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9334e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 68/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9348e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 69/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9342e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 70/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9325e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 71/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9316e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 72/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9340e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 73/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9336e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 74/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9344e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 75/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9318e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 76/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9326e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 77/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9338e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 78/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9361e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 79/100\n",
            "545/545 [==============================] - 9s 17ms/step - loss: 7.9343e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 80/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9385e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 81/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9369e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 82/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9342e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 83/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9350e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 84/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9324e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 85/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9316e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 86/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9339e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 87/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9327e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 88/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9325e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 89/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9318e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 90/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9323e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 91/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9349e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 92/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9372e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 93/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9342e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 94/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9336e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 95/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9333e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 96/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9346e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 97/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9353e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 98/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9353e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 99/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9357e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n",
            "Epoch 100/100\n",
            "545/545 [==============================] - 9s 16ms/step - loss: 7.9356e-05 - accuracy: 0.0068 - val_loss: 8.0460e-05 - val_accuracy: 0.0071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7jkorZCoSBk",
        "colab_type": "text"
      },
      "source": [
        "The model might have a case of vanishing gradient problem, and it's underfitting (plateauing loss) --> increase cells / increase layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKdAkEgho-V5",
        "colab_type": "code",
        "outputId": "87d68a14-300c-449b-80e6-85e5720dc5ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist.history[\"loss\"], color=\"r\")\n",
        "plt.plot(hist.history[\"val_loss\"], color=\"b\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2c686328d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkPphhKNjyW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = LSTM(latent_dim, return_sequences=True, return_state=True)(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSlDzQn_oEI6",
        "colab_type": "code",
        "outputId": "b340dfe3-02bf-4402-f8fc-6d27b48af29e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    input_seq = np.expand_dims(input_seq,axis=-1)\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1,1,1))\n",
        "    target_seq[0, 0, 0] = vocab_size_y - 2\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = []\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = output_tokens[:,-1,:]\n",
        "        decoded_sentence.append(sampled_token_index[0][0])\n",
        "\n",
        "        if (sampled_token_index == vocab_size_y - 1 or len(decoded_sentence) > MAXLEN):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1,1,1))\n",
        "        target_seq[0,0,0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "    pred = token_y.decode([int(i) for i in decoded_sentence])\n",
        "    return pred\n",
        "index_try = 33\n",
        "input_seq = inputs_1[index_try: index_try + 1]\n",
        "pred = decode_sequence(input_seq)\n",
        "print('-')\n",
        "print('Input sentence:', token_x.decode(np.squeeze(inputs_1[index_try], axis=-1)))\n",
        "print('Decoded sentence:', pred)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: ill just fetch my galoshes\n",
            "Decoded sentence: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBrexeMjFF2k",
        "colab_type": "text"
      },
      "source": [
        "I can't really tell whether the model sucks or the prediction sucks, but I am going to add an embedding layer in between to help //\n",
        "https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py"
      ]
    }
  ]
}