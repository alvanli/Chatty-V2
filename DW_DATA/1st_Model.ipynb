{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1st_Model",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrF-_9eHxNGh",
        "colab_type": "code",
        "outputId": "2456124c-b2a9-459c-8661-920997251c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%cd ..\n",
        "%cd root\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJbJM72pxg1O",
        "colab_type": "code",
        "outputId": "42bc05ac-6d03-42f6-9c6d-b276c65c5041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"dwdata_2.csv\")\n",
        "df.head()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>X_1</th>\n",
              "      <th>Y_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>im pleased you two are so impressed i find the...</td>\n",
              "      <td>yes i suppose the atmosphere is rather rancid</td>\n",
              "      <td>im pleased you two are so impressed i find the...</td>\n",
              "      <td>yes i suppose the atmosphere is rather rancid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>hes let us go</td>\n",
              "      <td>he needs to move his tardis into the circle of...</td>\n",
              "      <td>hes let us go</td>\n",
              "      <td>he needs to move his tardis into the circle of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>without special treatment it would have spread...</td>\n",
              "      <td>ah thats nice</td>\n",
              "      <td>without special treatment it would have spread...</td>\n",
              "      <td>ah thats nice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>when i came back with these satellite reports ...</td>\n",
              "      <td>nothings been moved</td>\n",
              "      <td>when i came back with these satellite reports ...</td>\n",
              "      <td>nothings been moved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>there it is but he could be anywhere</td>\n",
              "      <td>fair exchange is no robbery the masters tempor...</td>\n",
              "      <td>there it is but he could be anywhere</td>\n",
              "      <td>fair exchange is no robbery the masters tempor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                                Y_1\n",
              "0           0  ...      yes i suppose the atmosphere is rather rancid\n",
              "1           1  ...  he needs to move his tardis into the circle of...\n",
              "2           2  ...                                      ah thats nice\n",
              "3           3  ...                                nothings been moved\n",
              "4           4  ...  fair exchange is no robbery the masters tempor...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGnNmu5Vx6t6",
        "colab_type": "text"
      },
      "source": [
        "Punctuation has already been removed, all words are lowered. I suppose stemming would not be a nice idea unless we preprocess the data whenever a user enters something. Let's build a simple model then!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38My9GYry_WY",
        "colab_type": "code",
        "outputId": "d67dc4df-b379-406f-f77a-91d3adacd5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "import numpy as np\n",
        "words_x = [\"\"]\n",
        "words_y = [\"\"]\n",
        "def word_to_list_x(x):\n",
        "  words_x.append(str(x).split(\" \"))\n",
        "def word_to_list_y(x):\n",
        "  words_y.append(str(x).split(\" \"))\n",
        "df.X.apply(word_to_list_x)\n",
        "df.Y.apply(word_to_list_y)\n",
        "words_x1 = [val for sublist in words_x for val in sublist]\n",
        "words_y1 = [val for sublist in words_y for val in sublist]\n",
        "words_x = set(words_x1)\n",
        "words_y = set(words_y1)\n",
        "print(\"X:\")\n",
        "print(\"   # of unique words: {}, Total Words: {}\".format(len(words_x), len(words_x1)))\n",
        "print(\"Y: \")\n",
        "print(\"   # of unique words: {}, Total Words: {}\".format(len(words_y), len(words_y1)))\n",
        "print(\"Combined: \")\n",
        "print(\"   # of unique words: {}, Total Words: {}\".format(len(set(list(words_y)+list(words_x))), len(words_y1+words_x1)))\n",
        "print(\"Total Training Size: {}\".format(len(df)))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X:\n",
            "   # of unique words: 30793, Total Words: 725045\n",
            "Y: \n",
            "   # of unique words: 37657, Total Words: 939351\n",
            "Combined: \n",
            "   # of unique words: 53382, Total Words: 1664396\n",
            "Total Training Size: 76543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvKQJeUA1UiK",
        "colab_type": "code",
        "outputId": "160d90cd-aaba-41fb-d41b-32ce52ca10b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "pd.Series(words_y1+words_x1).value_counts()[20:30].plot.bar()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3608269278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOY0X-IS4h1L",
        "colab_type": "text"
      },
      "source": [
        "Max words per sentence is 253 as per the last notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgXzxnPh5Rw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "def remove_num_X(x):\n",
        "  x = str(x)\n",
        "  result = ''.join([i for i in x if not i.isdigit()])\n",
        "  X.append(result)\n",
        "def remove_num_Y(x):\n",
        "  x = str(x)\n",
        "  result = ''.join([i for i in x if not i.isdigit()])\n",
        "  Y.append(result)\n",
        "df.X.apply(remove_num_X)\n",
        "df.Y.apply(remove_num_Y)\n",
        "df[\"X_1\"] = X\n",
        "df[\"Y_1\"] = Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFYXMy0U7gEC",
        "colab_type": "code",
        "outputId": "982af44b-102b-4fa3-b7ab-aa21b23be005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "word_freq = pd.Series(words_y1+words_x1).value_counts()\n",
        "print(\"Total Number of Unique Words Before: {}\".format(len(word_freq)))\n",
        "ignored = word_freq[word_freq<=4]\n",
        "word_freq = word_freq[word_freq>4]\n",
        "print(\"Total Number of Unique Words After: {}\".format(len(word_freq)))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of Unique Words Before: 53382\n",
            "Total Number of Unique Words After: 11699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwYpPQFn7HMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ignored = list(ignored)\n",
        "X_1 = []\n",
        "Y_1 = []\n",
        "def filter_ignored_x(x):\n",
        "  each_word = x.split(\" \")\n",
        "  result = \" \".join([i for i in each_word if i not in ignored])\n",
        "  X_1.append(result)\n",
        "def filter_ignored_y(x):\n",
        "  each_word = x.split(\" \")\n",
        "  result = \" \".join([i for i in each_word if i not in ignored])\n",
        "  Y_1.append(result)\n",
        "df[\"X_1\"].apply(filter_ignored_x)\n",
        "df[\"Y_1\"].apply(filter_ignored_y)\n",
        "df[\"X_1\"] = X_1\n",
        "df[\"Y_1\"] = Y_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDnQfjxK6DTV",
        "colab_type": "code",
        "outputId": "c1cc0205-96b5-4dd7-a7d7-234dff236a0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>X_1</th>\n",
              "      <th>Y_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>im pleased you two are so impressed i find the...</td>\n",
              "      <td>yes i suppose the atmosphere is rather rancid</td>\n",
              "      <td>im pleased you two are so impressed i find the...</td>\n",
              "      <td>yes i suppose the atmosphere is rather rancid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>hes let us go</td>\n",
              "      <td>he needs to move his tardis into the circle of...</td>\n",
              "      <td>hes let us go</td>\n",
              "      <td>he needs to move his tardis into the circle of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>without special treatment it would have spread...</td>\n",
              "      <td>ah thats nice</td>\n",
              "      <td>without special treatment it would have spread...</td>\n",
              "      <td>ah thats nice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>when i came back with these satellite reports ...</td>\n",
              "      <td>nothings been moved</td>\n",
              "      <td>when i came back with these satellite reports ...</td>\n",
              "      <td>nothings been moved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>there it is but he could be anywhere</td>\n",
              "      <td>fair exchange is no robbery the masters tempor...</td>\n",
              "      <td>there it is but he could be anywhere</td>\n",
              "      <td>fair exchange is no robbery the masters tempor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                                Y_1\n",
              "0           0  ...      yes i suppose the atmosphere is rather rancid\n",
              "1           1  ...  he needs to move his tardis into the circle of...\n",
              "2           2  ...                                      ah thats nice\n",
              "3           3  ...                                nothings been moved\n",
              "4           4  ...  fair exchange is no robbery the masters tempor...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3w8VNTxx5jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 255\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "all_xy = df[\"X_1\"]\n",
        "all_xy = all_xy.append(df[\"Y_1\"])\n",
        "TO = Tokenizer(num_words=20000)\n",
        "TO.fit_on_texts(all_xy)\n",
        "X_train = TO.texts_to_sequences(df[\"X_1\"])\n",
        "Y_train = TO.texts_to_sequences(df[\"Y_1\"])\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "Y_train = pad_sequences(Y_train, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZButX2m06zDu",
        "colab_type": "code",
        "outputId": "31527c46-3ea1-4a55-a213-57b6676d4419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(76543, 255)\n",
            "(76543, 255)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgZ2um2G8NCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"dwdata_2.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDaSN6Ner7AR",
        "colab_type": "text"
      },
      "source": [
        "//"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_wFd3bCr7l6",
        "colab_type": "text"
      },
      "source": [
        "//"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfDBdyeEt-Hl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b12ea10b-b7d9-4287-9427-e421e8d1921f"
      },
      "source": [
        "%cd ..\n",
        "%cd root"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfXv1AqrFmKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"dwdata_2.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Og596kpTf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Input, Add, Dense, Flatten, Bidirectional, GlobalMaxPool1D, Dropout\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cSuHVfgu_q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.fillna('', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQiHtFr7ugT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df[\"X_1\"].tolist()\n",
        "Y = df[\"Y_1\"].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65RtNFLwrfLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_X = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    X, target_vocab_size = 1e15\n",
        ")\n",
        "tokenizer_Y = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    Y, target_vocab_size = 1e15\n",
        ")\n",
        "vocab_size_en = tokenizer_X.vocab_size+2\n",
        "vocab_size_pl = tokenizer_Y.vocab_size+2\n",
        "\n",
        "inputs = [[vocab_size_en-2] + tokenizer_X.encode(sentence) + [vocab_size_en-1] for sentence in df[\"X_1\"]]\n",
        "outputs = [[vocab_size_pl-2] + tokenizer_Y.encode(sentence) + [vocab_size_pl-1] for sentence in df[\"Y_1\"]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMtqztbuwcid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAXLEN = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR5Ls7tuwt6P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "76d3f411-35a2-493a-9e4f-acb4d544c12b"
      },
      "source": [
        "vocab_size_en"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37092"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awGygaz6tLds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove long sentences\n",
        "id2remove = [count for count, sentence in enumerate(inputs) if len(sentence) > MAXLEN]\n",
        "for idx in reversed(id2remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "id2remove = [count for count, sentence in enumerate(outputs) if len(sentence) > MAXLEN]\n",
        "for idx in reversed(id2remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X96mjGMAxVSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "01b7cefc-bd7e-4a1b-91d1-167c8807a821"
      },
      "source": [
        "print(\"{},{}\".format(len(inputs), len(outputs)))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "76476,76476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPsM0vRk1rGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls = [x.split(\" \") for x in X]\n",
        "ls =  [item for sublist in ls for item in sublist]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHfl9Qsu2xox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b36c0dea-8827-4b96-835d-231d379d28e1"
      },
      "source": [
        "len(set(ls))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30639"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIcBI8XVuJQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X_train = pad_sequences(inputs, value = 0, maxlen=MAXLEN, padding=\"post\")\n",
        "Y_train = pad_sequences(outputs, value = 0, maxlen=MAXLEN, padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQqUwbaAxr2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6932fa51-7401-4d5d-9009-049ae9e7b800"
      },
      "source": [
        "print(\"{},{}\".format(X_train.shape,Y_train.shape))\n",
        "inputs = X_train\n",
        "outputs = Y_train"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(76476, 100),(76476, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13gbZ0fPuP9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH = 64\n",
        "BUFFER = 20000\n",
        "ds = tf.data.Dataset.from_tensor_slices((inputs,outputs))\n",
        "ds = ds.cache()\n",
        "ds = ds.shuffle(BUFFER).batch(BATCH)\n",
        "ds = ds.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "migMzzOxK1oP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "class PositionalEncoding(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "  def get_angles(self,pos,i,d_model):\n",
        "    angles = 1 / np.power(10000.,(2*(i//2))/np.float32(d_model))\n",
        "    return pos * angles\n",
        "  def call(self,inputs):\n",
        "    seq_length = inputs.shape.as_list()[-2]\n",
        "    d_model = inputs.shape.as_list()[-1]\n",
        "    angles = self.get_angles(np.arange(seq_length)[:,np.newaxis],\n",
        "                             np.arange(d_model)[np.newaxis,:],\n",
        "                             d_model)\n",
        "    angles[:,0::2] = np.sin(angles[:,0::2])\n",
        "    angles[:,1::2] = np.cos(angles[:,1::2])\n",
        "    pos_encoding = angles[np.newaxis,...]\n",
        "    return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1EC6PzFL4uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention_computation(queries,keys,values,mask):\n",
        "  product = tf.matmul(queries,keys,transpose_b=True)\n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1],tf.float32)\n",
        "  scaled_product = product/tf.math.sqrt(tf.cast(keys_dim, tf.float32))\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask*-1e9)\n",
        "  attention = tf.matmul(tf.nn.softmax(scaled_product,axis=-1),values)\n",
        "  return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRR6cBZeMo-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "  def __init__(self,nb_proj):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "    assert self.d_model % self.nb_proj == 0\n",
        "    self.d_proj = self.d_model // self.nb_proj\n",
        "    self.query_lin = layers.Dense(units=self.d_model)\n",
        "    self.key_lin = layers.Dense(units = self.d_model)\n",
        "    self.value_lin = layers.Dense(units = self.d_model)\n",
        "    self.final_lin = layers.Dense(units = self.d_model)\n",
        "  def split_proj (self, inputs, batch_size):\n",
        "    shape = (batch_size,-1,self.nb_proj,self.d_proj)\n",
        "    splited_inputs = tf.reshape(inputs,shape=shape)\n",
        "    return tf.transpose(splited_inputs,perm=[0,2,1,3])\n",
        "  def call(self,queries,keys,values,mask):\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "    queries = self.query_lin(queries)\n",
        "    keys = self.key_lin(keys)\n",
        "    values = self.value_lin(values)\n",
        "\n",
        "    queries = self.split_proj(queries,batch_size)\n",
        "    keys = self.split_proj(keys,batch_size)\n",
        "    values = self.split_proj(values,batch_size)\n",
        "\n",
        "    attention = attention_computation(queries,keys,values,mask)\n",
        "    attention = tf.transpose(attention, perm=[0,2,1,3])\n",
        "    concat_attention = tf.reshape(attention, shape=(batch_size,-1,self.d_model))\n",
        "    outputs = self.final_lin(concat_attention)\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nk17W3zfmtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, ffn_units, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.ffn_units=ffn_units\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        self.multi_head_attention = MultiHeadAttention(self.n_heads)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dense_1 = layers.Dense(units=self.ffn_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,inputs,inputs,mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention+inputs)\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs+attention)\n",
        "        return outputs\n",
        "class Encoder(layers.Layer):\n",
        "    def __init__(self,n_layers,ffn_units,n_heads,dropout,vocab_size,d_model,name=\"encoder\"):\n",
        "        super().__init__(name=name)\n",
        "        self.n_layers = n_layers\n",
        "        self.d_model = d_model\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.enc_layers = [EncoderLayer(ffn_units, n_heads, dropout) for _ in range(self.n_layers)]\n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training=training)\n",
        "        for i in range(self.n_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "        return outputs\n",
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, ffn_units, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.ffn_units = ffn_units\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.n_heads)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.n_heads)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dense_1 = layers.Dense(units=self.ffn_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention+inputs)\n",
        "        attention_2 = self.multi_head_attention_2(attention, enc_outputs, enc_outputs, mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training=training)\n",
        "        attention_2 = self.norm_2(attention_2+attention)\n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training=training)\n",
        "        outputs = self.norm_3(outputs+attention_2)\n",
        "        return outputs\n",
        "class Decoder(layers.Layer):\n",
        "    def __init__(self,n_layers,ffn_units,n_heads,dropout,vocab_size,d_model,name=\"decoder\"):\n",
        "        super().__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.dec_layers = [DecoderLayer(ffn_units, n_heads, dropout) for _ in range(n_layers)]\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training=training)\n",
        "        for i in range(self.n_layers):\n",
        "            outputs = self.dec_layers[i](outputs, enc_outputs, mask_1, mask_2, training)\n",
        "        return outputs\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self,vocab_size_enc,vocab_size_dec,d_model,n_layers,FFN_units,nb_proj,dropout,name=\"transformer\"):\n",
        "    super(Transformer,self).__init__(name=name)\n",
        "    self.encoder = Encoder(n_layers,FFN_units,nb_proj,dropout,vocab_size_enc,d_model)\n",
        "    self.decoder = Decoder(n_layers,FFN_units,nb_proj,dropout,vocab_size_dec,d_model)\n",
        "    self.last_linear = layers.Dense(units = vocab_size_dec)\n",
        "  def create_padding_mask(self,seq):\n",
        "    mask = tf.cast(tf.math.equal(seq,0),tf.float32)\n",
        "    return mask[:,tf.newaxis,tf.newaxis,:]\n",
        "  def create_look_ahead_mask(self,seq):\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len,seq_len)),-1,0)\n",
        "    return look_ahead_mask\n",
        "  def call(self,enc_inputs,dec_inputs,training):\n",
        "    enc_mask = self.create_padding_mask(enc_inputs)\n",
        "    dec_mask_1 = tf.maximum(\n",
        "        self.create_padding_mask(dec_inputs),\n",
        "        self.create_look_ahead_mask(dec_inputs)\n",
        "    )\n",
        "    dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "    enc_outputs = self.encoder(enc_inputs,enc_mask,training)\n",
        "    dec_outputs = self.decoder(dec_inputs,\n",
        "                               enc_outputs,\n",
        "                               dec_mask_1,\n",
        "                               dec_mask_2,\n",
        "                               training)\n",
        "    outputs = self.last_linear(dec_outputs)\n",
        "    return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9W6c9zIkHak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "D_MODEL = 128\n",
        "N_LAYERS = 4\n",
        "FFN_UNITS = 512\n",
        "NB_PROJ = 8\n",
        "DROPOUT = 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc = vocab_size_en,\n",
        "                          vocab_size_dec = vocab_size_pl,\n",
        "                          d_model = D_MODEL,\n",
        "                          n_layers = N_LAYERS,\n",
        "                          FFN_units = FFN_UNITS,\n",
        "                          nb_proj = NB_PROJ,\n",
        "                          dropout = DROPOUT)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction=\"none\")\n",
        "\n",
        "def loss_function(target,pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(target,0))\n",
        "  loss_ = loss_object(target,pred)\n",
        "  mask = tf.cast(mask,dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = \"train_accuracy\")\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__ (self,d_model, warmup_steps = 4000):\n",
        "    super(CustomSchedule,self).__init__()\n",
        "    self.d_model = tf.cast(d_model,tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "  def __call__ (self,step):\n",
        "    arg1 = tf.math.rsqrt(tf.cast(step, tf.float32))\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    return  tf.math.rsqrt(tf.cast(self.d_model, tf.float32)) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,beta_1 = 0.9, beta_2 = 0.98, epsilon=1e-9)\n",
        "checkpoint_path = \"\"\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt,checkpoint_path,max_to_keep = 5)\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Latest Checkpoint Restored\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBP-l_zMkoGH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b1fcc1f-95f1-496a-9fdd-fde3494353de"
      },
      "source": [
        "import time\n",
        "epochs = 4\n",
        "for epoch in range(epochs):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    for (batch, (enc_inputs, targets)) in enumerate(ds):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1, ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch {}\".format(time.time()-start))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 1.6541 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 1.4269 Accuracy 0.0000\n",
            "Epoch 1 Batch 100 Loss 1.4041 Accuracy 0.0043\n",
            "Epoch 1 Batch 150 Loss 1.3992 Accuracy 0.0063\n",
            "Epoch 1 Batch 200 Loss 1.3833 Accuracy 0.0072\n",
            "Epoch 1 Batch 250 Loss 1.3577 Accuracy 0.0078\n",
            "Epoch 1 Batch 300 Loss 1.3292 Accuracy 0.0082\n",
            "Epoch 1 Batch 350 Loss 1.2902 Accuracy 0.0084\n",
            "Epoch 1 Batch 400 Loss 1.2560 Accuracy 0.0087\n",
            "Epoch 1 Batch 450 Loss 1.2211 Accuracy 0.0088\n",
            "Epoch 1 Batch 500 Loss 1.1890 Accuracy 0.0089\n",
            "Epoch 1 Batch 550 Loss 1.1626 Accuracy 0.0090\n",
            "Epoch 1 Batch 600 Loss 1.1380 Accuracy 0.0091\n",
            "Epoch 1 Batch 650 Loss 1.1182 Accuracy 0.0092\n",
            "Epoch 1 Batch 700 Loss 1.1000 Accuracy 0.0095\n",
            "Epoch 1 Batch 750 Loss 1.0838 Accuracy 0.0098\n",
            "Epoch 1 Batch 800 Loss 1.0701 Accuracy 0.0101\n",
            "Epoch 1 Batch 850 Loss 1.0575 Accuracy 0.0104\n",
            "Epoch 1 Batch 900 Loss 1.0453 Accuracy 0.0107\n",
            "Epoch 1 Batch 950 Loss 1.0336 Accuracy 0.0110\n",
            "Epoch 1 Batch 1000 Loss 1.0227 Accuracy 0.0113\n",
            "Epoch 1 Batch 1050 Loss 1.0128 Accuracy 0.0117\n",
            "Epoch 1 Batch 1100 Loss 1.0036 Accuracy 0.0120\n",
            "Epoch 1 Batch 1150 Loss 0.9951 Accuracy 0.0123\n",
            "Saving checkpoint for epoch 1 at ckpt-1\n",
            "Time taken for 1 epoch 654.8329200744629\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 0.6750 Accuracy 0.0208\n",
            "Epoch 2 Batch 50 Loss 0.8019 Accuracy 0.0203\n",
            "Epoch 2 Batch 100 Loss 0.7989 Accuracy 0.0204\n",
            "Epoch 2 Batch 150 Loss 0.7983 Accuracy 0.0208\n",
            "Epoch 2 Batch 200 Loss 0.7962 Accuracy 0.0210\n",
            "Epoch 2 Batch 250 Loss 0.7944 Accuracy 0.0212\n",
            "Epoch 2 Batch 300 Loss 0.7928 Accuracy 0.0213\n",
            "Epoch 2 Batch 350 Loss 0.7907 Accuracy 0.0214\n",
            "Epoch 2 Batch 400 Loss 0.7880 Accuracy 0.0215\n",
            "Epoch 2 Batch 450 Loss 0.7853 Accuracy 0.0216\n",
            "Epoch 2 Batch 500 Loss 0.7818 Accuracy 0.0217\n",
            "Epoch 2 Batch 550 Loss 0.7784 Accuracy 0.0218\n",
            "Epoch 2 Batch 600 Loss 0.7768 Accuracy 0.0219\n",
            "Epoch 2 Batch 650 Loss 0.7760 Accuracy 0.0219\n",
            "Epoch 2 Batch 700 Loss 0.7737 Accuracy 0.0220\n",
            "Epoch 2 Batch 750 Loss 0.7721 Accuracy 0.0221\n",
            "Epoch 2 Batch 800 Loss 0.7708 Accuracy 0.0222\n",
            "Epoch 2 Batch 850 Loss 0.7690 Accuracy 0.0222\n",
            "Epoch 2 Batch 900 Loss 0.7688 Accuracy 0.0223\n",
            "Epoch 2 Batch 950 Loss 0.7671 Accuracy 0.0223\n",
            "Epoch 2 Batch 1000 Loss 0.7656 Accuracy 0.0224\n",
            "Epoch 2 Batch 1050 Loss 0.7643 Accuracy 0.0225\n",
            "Epoch 2 Batch 1100 Loss 0.7629 Accuracy 0.0225\n",
            "Epoch 2 Batch 1150 Loss 0.7612 Accuracy 0.0226\n",
            "Saving checkpoint for epoch 2 at ckpt-2\n",
            "Time taken for 1 epoch 662.3430633544922\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 0.7870 Accuracy 0.0292\n",
            "Epoch 3 Batch 50 Loss 0.7349 Accuracy 0.0251\n",
            "Epoch 3 Batch 100 Loss 0.7280 Accuracy 0.0248\n",
            "Epoch 3 Batch 150 Loss 0.7246 Accuracy 0.0247\n",
            "Epoch 3 Batch 200 Loss 0.7305 Accuracy 0.0248\n",
            "Epoch 3 Batch 250 Loss 0.7321 Accuracy 0.0249\n",
            "Epoch 3 Batch 300 Loss 0.7326 Accuracy 0.0250\n",
            "Epoch 3 Batch 350 Loss 0.7314 Accuracy 0.0250\n",
            "Epoch 3 Batch 400 Loss 0.7288 Accuracy 0.0250\n",
            "Epoch 3 Batch 450 Loss 0.7303 Accuracy 0.0251\n",
            "Epoch 3 Batch 500 Loss 0.7306 Accuracy 0.0252\n",
            "Epoch 3 Batch 550 Loss 0.7274 Accuracy 0.0252\n",
            "Epoch 3 Batch 600 Loss 0.7267 Accuracy 0.0252\n",
            "Epoch 3 Batch 650 Loss 0.7263 Accuracy 0.0253\n",
            "Epoch 3 Batch 700 Loss 0.7252 Accuracy 0.0253\n",
            "Epoch 3 Batch 750 Loss 0.7242 Accuracy 0.0253\n",
            "Epoch 3 Batch 800 Loss 0.7223 Accuracy 0.0254\n",
            "Epoch 3 Batch 850 Loss 0.7206 Accuracy 0.0254\n",
            "Epoch 3 Batch 900 Loss 0.7206 Accuracy 0.0254\n",
            "Epoch 3 Batch 950 Loss 0.7199 Accuracy 0.0254\n",
            "Epoch 3 Batch 1000 Loss 0.7197 Accuracy 0.0255\n",
            "Epoch 3 Batch 1050 Loss 0.7183 Accuracy 0.0255\n",
            "Epoch 3 Batch 1100 Loss 0.7178 Accuracy 0.0255\n",
            "Epoch 3 Batch 1150 Loss 0.7173 Accuracy 0.0255\n",
            "Saving checkpoint for epoch 3 at ckpt-3\n",
            "Time taken for 1 epoch 648.0916378498077\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 0.5668 Accuracy 0.0237\n",
            "Epoch 4 Batch 50 Loss 0.6966 Accuracy 0.0270\n",
            "Epoch 4 Batch 100 Loss 0.6972 Accuracy 0.0270\n",
            "Epoch 4 Batch 150 Loss 0.6951 Accuracy 0.0269\n",
            "Epoch 4 Batch 200 Loss 0.6982 Accuracy 0.0271\n",
            "Epoch 4 Batch 250 Loss 0.6991 Accuracy 0.0271\n",
            "Epoch 4 Batch 300 Loss 0.6981 Accuracy 0.0270\n",
            "Epoch 4 Batch 350 Loss 0.6967 Accuracy 0.0270\n",
            "Epoch 4 Batch 400 Loss 0.6966 Accuracy 0.0270\n",
            "Epoch 4 Batch 450 Loss 0.6971 Accuracy 0.0271\n",
            "Epoch 4 Batch 500 Loss 0.6975 Accuracy 0.0271\n",
            "Epoch 4 Batch 550 Loss 0.6956 Accuracy 0.0271\n",
            "Epoch 4 Batch 600 Loss 0.6954 Accuracy 0.0272\n",
            "Epoch 4 Batch 650 Loss 0.6956 Accuracy 0.0272\n",
            "Epoch 4 Batch 700 Loss 0.6942 Accuracy 0.0272\n",
            "Epoch 4 Batch 750 Loss 0.6943 Accuracy 0.0273\n",
            "Epoch 4 Batch 800 Loss 0.6948 Accuracy 0.0273\n",
            "Epoch 4 Batch 850 Loss 0.6931 Accuracy 0.0273\n",
            "Epoch 4 Batch 900 Loss 0.6923 Accuracy 0.0273\n",
            "Epoch 4 Batch 950 Loss 0.6920 Accuracy 0.0274\n",
            "Epoch 4 Batch 1000 Loss 0.6910 Accuracy 0.0274\n",
            "Epoch 4 Batch 1050 Loss 0.6899 Accuracy 0.0274\n",
            "Epoch 4 Batch 1100 Loss 0.6892 Accuracy 0.0274\n",
            "Epoch 4 Batch 1150 Loss 0.6897 Accuracy 0.0274\n",
            "Saving checkpoint for epoch 4 at ckpt-4\n",
            "Time taken for 1 epoch 632.9655010700226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwvKHjaQrhCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = inp_sentence.lower()\n",
        "    inp_sentence = [vocab_size_en - 2] + tokenizer_X.encode(inp_sentence) + [vocab_size_en - 1]\n",
        "    encoder_inp = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([vocab_size_pl - 2], axis=0)\n",
        "    for _ in range(MAXLEN):\n",
        "        predictions = transformer(encoder_inp, output, False)\n",
        "        prediction = predictions[:, -1:, :]\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.float32)\n",
        "\n",
        "        if predicted_id == vocab_size_pl-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        print(output)\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEVRkn6trjye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "\n",
        "    predicted_sentence = tokenizer_Y.decode([i for i in output if i < vocab_size_pl-2])\n",
        "\n",
        "    print(\"Input sentence: {}\".format(sentence))\n",
        "    print(\"Translated sentence: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5-hrHnbHTTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5dda1fbb-5666-4537-deb3-8413e03375d1"
      },
      "source": [
        "translate(\"Hello Doctor I am in danger\")"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sentence: Hello Doctor I am in danger\n",
            "Translated sentence: oh no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiY1_a45OoKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "e85a1b8f-9a1a-4418-a4b5-85aaf1bff0fd"
      },
      "source": [
        "translate(\"Are you alright mate\")"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[44307]], shape=(1, 1), dtype=int32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-fb9832f2b0fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Are you alright mate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-127-d8a5daeae2c9>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredicted_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mvocab_size_pl\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-137-f80efd2e86ef>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(inp_sentence)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1604\u001b[0m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[1;32m   1605\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute ConcatV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:ConcatV2] name: concat"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsWCE_GSIUfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/Alt3rius/EN-PL-Transformer/blob/36990ac16478387267e8aa2de90d493fce2fdace/Transformer.ipynb\n",
        "#https://arxiv.org/pdf/1706.03762.pdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DejbaJuuDto4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sab1pGFaC-ER",
        "colab_type": "text"
      },
      "source": [
        "use embedding, use classic decoder encoder"
      ]
    }
  ]
}