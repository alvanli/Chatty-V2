{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIqge7r5GOxX"
   },
   "source": [
    "In this notebook, I will mainly add embeddings. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1468,
     "status": "ok",
     "timestamp": 1587313182638,
     "user": {
      "displayName": "Alvin Li",
      "photoUrl": "",
      "userId": "06437038450383299514"
     },
     "user_tz": 240
    },
    "id": "rGqN2bd-GGK0",
    "outputId": "9b644338-3820-401a-b57c-3db270a5313c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76543\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"C://DATA//train//38_chattyv2//Chatty-V2//DW_DATA//dwdata_3.csv\")\n",
    "df.head()\n",
    "df.fillna('', inplace=True)\n",
    "print(len(df))\n",
    "x_1 = df[\"xx\"].tolist()\n",
    "y_1 = df[\"yy\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10516,
     "status": "ok",
     "timestamp": 1587313197415,
     "user": {
      "displayName": "Alvin Li",
      "photoUrl": "",
      "userId": "06437038450383299514"
     },
     "user_tz": 240
    },
    "id": "_3X_nPPcBSMY",
    "outputId": "03d902b2-f2e1-4902-a42a-9114fd43364a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of word vectors are:  400000\n"
     ]
    }
   ],
   "source": [
    "word2vec_index = {}\n",
    "f = open(\"C://DATA//train//38_chattyv2//Chatty-V2//glove.6B.100d.txt\",encoding=\"utf8\")\n",
    "for line in f:\n",
    "    words = line.split()\n",
    "    word = words[0]\n",
    "    index = np.asarray(words[1:], dtype=\"float32\")\n",
    "    word2vec_index[word] = index\n",
    "f.close()\n",
    "\n",
    "print(\"The number of word vectors are: \", len(word2vec_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30530,
     "status": "ok",
     "timestamp": 1587313220060,
     "user": {
      "displayName": "Alvin Li",
      "photoUrl": "",
      "userId": "06437038450383299514"
     },
     "user_tz": 240
    },
    "id": "jJE_MBJCq63o",
    "outputId": "26a53b93-3fa3-4f2f-d68c-e7374794f99b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76543,76543\n",
      "(76543, 50),(76543, 50)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "token = tfds.features.text.SubwordTextEncoder.build_from_corpus(x_1+y_1, target_vocab_size=1e12)\n",
    "\n",
    "vocab_size = token.vocab_size + 2 # adding the start and end characters\n",
    "\n",
    "inputs = [token.encode(sentence) for sentence in x_1]\n",
    "outputs = [[vocab_size - 2] + token.encode(sentence) + [vocab_size - 1] for sentence in y_1]\n",
    "print(\"{},{}\".format(len(inputs),len(outputs)))\n",
    "df[\"sent_len\"] = [len(sent) for sent in inputs]\n",
    "MAXLEN = 50\n",
    "idx_to_shorten = [count for count, sent in enumerate(inputs) if len(sent) > MAXLEN]\n",
    "for idx in idx_to_shorten:\n",
    "    inputs[idx] = inputs[idx][:MAXLEN]\n",
    "    outputs[idx] = outputs[idx][:MAXLEN]\n",
    "idx_to_shorten = []\n",
    "idx_to_shorten = [count for count, sent in enumerate(outputs) if len(sent) > MAXLEN]\n",
    "for idx in idx_to_shorten:\n",
    "    inputs[idx] = inputs[idx][:MAXLEN]\n",
    "    outputs[idx] = outputs[idx][:MAXLEN]\n",
    "# deleted min len because short convos are still good :)\n",
    "inputs = pad_sequences(inputs, value=0, padding=\"post\", maxlen=MAXLEN)\n",
    "outputs = pad_sequences(outputs, value=0, padding=\"post\", maxlen=MAXLEN)\n",
    "print(\"{},{}\".format(inputs.shape,outputs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1587311834654,
     "user": {
      "displayName": "Alvin Li",
      "photoUrl": "",
      "userId": "06437038450383299514"
     },
     "user_tz": 240
    },
    "id": "fm0hU620H-dE",
    "outputId": "0facf905-266d-4696-ef00-17882b2ee147"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>X_1</th>\n",
       "      <th>Y_1</th>\n",
       "      <th>xx</th>\n",
       "      <th>yy</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>im pleased you two are so impressed i find the...</td>\n",
       "      <td>yes i suppose the atmosphere is rather rancid</td>\n",
       "      <td>im pleased you two are so impressed i find the...</td>\n",
       "      <td>yes i suppose the atmosphere is rather rancid</td>\n",
       "      <td>im pleased you two are so impressed i find the...</td>\n",
       "      <td>yes i suppose the atmosphere is rather</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hes let us go</td>\n",
       "      <td>he needs to move his tardis into the circle of...</td>\n",
       "      <td>hes let us go</td>\n",
       "      <td>he needs to move his tardis into the circle of...</td>\n",
       "      <td>he is let us go</td>\n",
       "      <td>he needs to move his tardis into the circle of...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>without special treatment it would have spread...</td>\n",
       "      <td>ah thats nice</td>\n",
       "      <td>without special treatment it would have spread...</td>\n",
       "      <td>ah thats nice</td>\n",
       "      <td>without special treatment it would have spread...</td>\n",
       "      <td>ah thats nice</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>when i came back with these satellite reports ...</td>\n",
       "      <td>nothings been moved</td>\n",
       "      <td>when i came back with these satellite reports ...</td>\n",
       "      <td>nothings been moved</td>\n",
       "      <td>when i came back with the ise satellite report...</td>\n",
       "      <td>nothings been moved</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>there it is but he could be anywhere</td>\n",
       "      <td>fair exchange is no robbery the masters tempor...</td>\n",
       "      <td>there it is but he could be anywhere</td>\n",
       "      <td>fair exchange is no robbery the masters tempor...</td>\n",
       "      <td>there it is but he could be anywhere</td>\n",
       "      <td>fair exchange is no robbery the masters tempor...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ... sent_len\n",
       "0           0  ...       11\n",
       "1           1  ...        5\n",
       "2           2  ...       11\n",
       "3           3  ...       19\n",
       "4           4  ...        8\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6e82tsSq8FY"
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sentence in (x_1 + y_1):\n",
    "    words = sentence.split(\" \")\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 817,
     "status": "ok",
     "timestamp": 1587313230827,
     "user": {
      "displayName": "Alvin Li",
      "photoUrl": "",
      "userId": "06437038450383299514"
     },
     "user_tz": 240
    },
    "id": "glBunzFnrAVD",
    "outputId": "51467c49-726e-4a3f-9c30-915c3d4dc408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In GLOVE: 11558, Not in GLOVE: 703\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "count = 0\n",
    "count_1 = 0\n",
    "not_glove = []\n",
    "for i, word in enumerate(token.subwords):\n",
    "    if (word[-1] == \"_\"):\n",
    "        word = word[:-1]\n",
    "    embedding_vector = word2vec_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        count_1 += 1\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(100)\n",
    "        not_glove.append(word)\n",
    "        count += 1\n",
    "print(\"In GLOVE: {}, Not in GLOVE: {}\".format(count_1, count))\n",
    "# HALF OF THE WORDS ARE NOT IN GLOVE WHYYYYYYYYYYY (glove.6B.100d)\n",
    "# The updated version has 92% of the vocab in glove! Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 770,
     "status": "ok",
     "timestamp": 1587313240593,
     "user": {
      "displayName": "Alvin Li",
      "photoUrl": "",
      "userId": "06437038450383299514"
     },
     "user_tz": 240
    },
    "id": "mbQo0e7vrVvE",
    "outputId": "202c6390-c6fb-49d2-fe03-42fc8d30c145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76543, 50),(76543, 50)\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "inputs_1 = inputs\n",
    "inputs_2 = outputs\n",
    "outputs_1 = outputs\n",
    "outputs_1 = shift(outputs_1, [0,-1], cval=0)\n",
    "print(\"{},{}\".format(inputs_1.shape,outputs_1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1587313243386,
     "user": {
      "displayName": "Alvin Li",
      "photoUrl": "",
      "userId": "06437038450383299514"
     },
     "user_tz": 240
    },
    "id": "Jj6dJIVuNu8F",
    "outputId": "89b2035c-bed2-43de-be6a-4ea221ab8791"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12520"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "uVeWwGFgN2Jh",
    "outputId": "37831c0c-4474-4a87-c301-85fbe1174aeb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f126229f584cbd9da29d3ca9d4693a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=76543), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "x1 = []\n",
    "x2 = []\n",
    "y1 = []\n",
    "for sentence in tqdm(inputs_1):\n",
    "  sentence1 = []\n",
    "  for index in sentence:\n",
    "    onehot = np.zeros((vocab_size))\n",
    "    onehot[index] = 1\n",
    "    sentence1.append(onehot)\n",
    "  x1.append(sentence1)\n",
    "for sentence in tqdm(inputs_2):\n",
    "  sentence1 = []\n",
    "  for index in sentence:\n",
    "    onehot = np.zeros((vocab_size))\n",
    "    onehot[index] = 1\n",
    "    sentence1.append(onehot)\n",
    "  x2.append(sentence1)\n",
    "for sentence in tqdm(outputs_1):\n",
    "  sentence1 = []\n",
    "  for index in sentence:\n",
    "    onehot = np.zeros((vocab_size))\n",
    "    onehot[index] = 1\n",
    "    sentence1.append(onehot)\n",
    "  y1.append(sentence1)\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)\n",
    "y1 = np.array(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 859,
     "status": "ok",
     "timestamp": 1587313068252,
     "user": {
      "displayName": "Alvin Li",
      "photoUrl": "",
      "userId": "06437038450383299514"
     },
     "user_tz": 240
    },
    "id": "e_Jt0mulOwON",
    "outputId": "c3de6761-e7a3-482b-9abd-410b44a3ca59"
   },
   "outputs": [],
   "source": [
    "print(\"{},{},{}\".format(x1.shape,x2.shape,y1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(x1, open(\"x1_1.pkl\", \"w\"))\n",
    "pickle.dump(x2, open(\"x2_1.pkl\", \"w\"))\n",
    "pickle.dump(y1, open(\"y1_1.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MjpK66GrfpI"
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Bidirectional, Dropout, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.models import Model\n",
    "latent_dim = 256\n",
    "Shared_Embedding = Embedding(vocab_size,100,weights=[embedding_matrix],input_length=MAXLEN,trainable=True)\n",
    "\n",
    "encoder_inputs = Input(shape=(MAXLEN,), dtype='int32')\n",
    "encoder_embedding = Shared_Embedding(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state = True)(encoder_embedding)\n",
    "\n",
    "decoder_inputs = Input(shape=(MAXLEN,), dtype='int32')\n",
    "decoder_embedding = Shared_Embedding(decoder_inputs)\n",
    "decoder_outputs, _, _ = LSTM(latent_dim, return_state=True, return_sequences=True)(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "outputs_model = Dense(1, activation='softmax')(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs_model)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.05), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2820061,
     "status": "error",
     "timestamp": 1587307404239,
     "user": {
      "displayName": "Alvin Li",
      "photoUrl": "",
      "userId": "06437038450383299514"
     },
     "user_tz": 240
    },
    "id": "u-tcJb1Zri46",
    "outputId": "7b588032-d36a-4113-87de-a6e6dca2cb29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1914/1914 [==============================] - 76s 40ms/step - loss: 386584.4062 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 2/100\n",
      "1914/1914 [==============================] - 76s 40ms/step - loss: 386592.7812 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 3/100\n",
      "1914/1914 [==============================] - 76s 40ms/step - loss: 386585.0938 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 4/100\n",
      "1914/1914 [==============================] - 76s 40ms/step - loss: 386589.5000 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 5/100\n",
      "1914/1914 [==============================] - 76s 40ms/step - loss: 386586.6562 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 6/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386601.4375 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 7/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386582.7500 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 8/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386584.4688 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 9/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386582.3750 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 10/100\n",
      "1914/1914 [==============================] - 76s 40ms/step - loss: 386581.2188 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 11/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386584.8438 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 12/100\n",
      "1914/1914 [==============================] - 76s 40ms/step - loss: 386584.1562 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 13/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386582.2500 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 14/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386591.3438 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 15/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386588.6875 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 16/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386585.2812 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 17/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386615.8125 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 18/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386580.2500 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 19/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386587.5000 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 20/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386585.8125 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 21/100\n",
      "1914/1914 [==============================] - 79s 41ms/step - loss: 386596.6562 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 22/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386587.6875 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 23/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386584.0625 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 24/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386591.5625 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 25/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386596.8750 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 26/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386585.4688 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 27/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386584.7188 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 28/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386599.8125 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 29/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386586.4375 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 30/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386582.2812 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 31/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386576.2500 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 32/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386587.3750 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 33/100\n",
      "1914/1914 [==============================] - 78s 41ms/step - loss: 386596.2500 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 34/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386594.9688 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 35/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386602.3750 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 36/100\n",
      "1914/1914 [==============================] - 77s 40ms/step - loss: 386580.2500 - accuracy: 0.0065 - val_loss: 386322.4688 - val_accuracy: 0.0063\n",
      "Epoch 37/100\n",
      " 839/1914 [============>.................] - ETA: 41s - loss: 387232.6250 - accuracy: 0.0065"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-dd15651c8f8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2nd_m.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit([inputs_1, inputs_2], outputs_1,batch_size=32,epochs=50,validation_split=0.2, verbose = 1)\n",
    "model.save(\"2nd_m.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IwBMGVqgtq2f"
   },
   "outputs": [],
   "source": [
    "# use bidirectional lstm next time?\n",
    "#https://github.com/oswaldoludwig/Adversarial-Learning-for-Generative-Conversational-Agents !!!!GAANNN!!!!\n",
    "# Put attention in as well!\n",
    "encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = LSTM(latent_dim, return_sequences=True, return_state=True)(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hQCUjwytzPg"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    input_seq = np.expand_dims(input_seq,axis=-1)\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1,1,1))\n",
    "    target_seq[0, 0, 0] = vocab_size_y - 2\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = output_tokens[:,-1,:]\n",
    "        decoded_sentence.append(sampled_token_index[0][0])\n",
    "\n",
    "        if (sampled_token_index == vocab_size_y - 1 or len(decoded_sentence) > MAXLEN):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1,1,1))\n",
    "        target_seq[0,0,0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    pred = token_y.decode([int(i) for i in decoded_sentence])\n",
    "    return pred\n",
    "index_try = 33\n",
    "input_seq = inputs_1[index_try: index_try + 1]\n",
    "pred = decode_sequence(input_seq)\n",
    "print('-')\n",
    "print('Input sentence:', token_x.decode(np.squeeze(inputs_1[index_try], axis=-1)))\n",
    "print('Decoded sentence:', pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNqE9QcEj9h76bWfGFgIuxI",
   "machine_shape": "hm",
   "name": "3_5_Model",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
