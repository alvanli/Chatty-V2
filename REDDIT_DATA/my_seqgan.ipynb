{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_seqgan",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsfxG3ZtiWgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "#https://github.com/BenStringer3/SeqGan/blob/31638d223de44a6ad1dfb884512c2fb0703a445c/pre_train_disc.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-l6BSlqVsdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discrim(object):\n",
        "  def __init__(self, vocab_size, embedding_dim, latent_dim, batch_size, seq_length):\n",
        "    super(Discrim, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.latent_dim = latent_dim\n",
        "    self.seq_length = seq_length\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.inputs = tf.keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(latent_dim, return_sequences = True, return_state = True, recurrent_initializer=\"glorot_uniform\")\n",
        "    self.dropout = tf.keras.layer.Dropout(0.2)\n",
        "    self.fc1 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    self.optim = tf.optimizers.Adam(learning_rate=0.01)\n",
        "    self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    self.model = self.build()\n",
        "  def build(self):\n",
        "    embed = self.embedding(self.inputs)\n",
        "    x, h, c = self.lstm(embed)\n",
        "    pred = self.fc(x)\n",
        "    model = tf.keras.Model(self.inputs, pred)\n",
        "    return model\n",
        "  def loss(self,real,fake):\n",
        "    real_loss = self.cross_entropy(tf.ones_like(real), real)\n",
        "    fake_loss = self.cross_entropy(tf.zeros_like(fake),fake)\n",
        "    total = real_loss + fake_loss\n",
        "    return total\n",
        "  def train_step(self,fake, real):\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      fake_pred = self.model(fake)\n",
        "      real_pred = self.model(real)\n",
        "      loss = self.loss(real_pred, fake_pred)\n",
        "    grad = disc_tape.gradient(loss, self.model.trainable_variables)\n",
        "    update = self.optim.apply_gradients(zip(grad, self.model.trainable_variables))\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS-abFT4iaB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Gen(object):\n",
        "  def __init__(self, vocab_size, embedding_dim, latent_dim, batch_size, seq_len, start_token):\n",
        "    super(Gen, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.latent_dim = latent_dim\n",
        "    self.seq_length = seq_length\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.start_token = tf.identity(tf.constant([start_token]*batch_size))\n",
        "    self.optim = tf.optimizers.Adam(learning_rate=0.01)\n",
        "    self.inputs = tf.keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(latent_dim, return_sequences = True, return_state = True, recurrent_initializer=\"glorot_uniform\")\n",
        "    self.dropout = tf.keras.layer.Dropout(0.2)\n",
        "    self.fc1 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.model = self.build()\n",
        "  def build(self):\n",
        "    embed = self.embedding(self.inputs)\n",
        "    x, h, c = self.lstm(embed)\n",
        "    pred = self.fc(x)\n",
        "    model = tf.keras.Model(self.inputs, pred)\n",
        "    return model\n",
        "  def generate(self, seq_len = None):\n",
        "    if seq_len is None:\n",
        "      seq_len = self.seq_length\n",
        "    sequence = tf.TensorArray(dtype=tf.int32, size=seq_len, dynamic_size = False, infer_shape=True)\n",
        "\n",
        "    def g_recurr(i, x_t0, sequence):\n",
        "      x_t0 = tf.reshape(x_t0, [self.batch_size, 1])\n",
        "      o_t = self.model(x_t0)\n",
        "      log_prob = tf.math.log(tf.nn.softmax(o_t))\n",
        "      x_t1 = tf.cast(tf.reshape(tf.random.categorical(log_prob, 1), [self.batch_size]), tf.int32)\n",
        "      gen_x = gen_x.write(i, x_t1)\n",
        "      return i + 1, x_t1, gen_x\n",
        "\n",
        "    gen_x = gen_x.write(0, self.start_token)\n",
        "\n",
        "    _, _,  self.gen_x = tf.while_loop(\n",
        "            cond=lambda i, _1, _2: i < seq_len,\n",
        "            body=_g_recurr,\n",
        "            loop_vars=(tf.constant(1, dtype=tf.int32),self.start_token,gen_x))\n",
        "    \n",
        "    self.gen_x = self.gen_x.stack()  # seq_length x batch_size\n",
        "    self.gen_x = tf.transpose(self.gen_x, perm=[1, 0])  # batch_size x seq_length\n",
        "    self.model.reset_states()\n",
        "    return self.gen_x\n",
        "  def gen_predictions(self, x, training=False): # x in token form [batch_size, seq_length]\n",
        "        g_predictions = tf.TensorArray(\n",
        "            dtype=tf.float32, size=self.sequence_length,\n",
        "            dynamic_size=False, infer_shape=True)\n",
        "\n",
        "        x_transposed = tf.cast(tf.transpose(x), dtype=tf.int32)\n",
        "        ta_x = tf.TensorArray(\n",
        "            dtype=tf.int32, size=self.sequence_length)\n",
        "        ta_x = ta_x.unstack(x_transposed)\n",
        "\n",
        "        def _pretrain_recurrence(i, x_t, g_predictions):\n",
        "            x_t = tf.reshape(x_t, [self.batch_size, 1])\n",
        "            o_t = self.model(x_t, training=training)\n",
        "            g_predictions = g_predictions.write(i, tf.nn.softmax(o_t))  # batch x vocab_size\n",
        "            x_tp1 = ta_x.read(i)\n",
        "            return i + 1, x_tp1, g_predictions\n",
        "\n",
        "        ta_x.write(0, self.start_token)\n",
        "        _, _, self.g_predictions = tf.while_loop(\n",
        "            cond=lambda i, _1, _2: i < self.sequence_length,\n",
        "            body=_pretrain_recurrence,\n",
        "            loop_vars=(tf.constant(1, dtype=tf.int32),\n",
        "                      ta_x.read(0),\n",
        "                        g_predictions))\n",
        "\n",
        "        self.g_predictions = tf.transpose(self.g_predictions.stack(), perm=[1, 0, 2])  # batch_size x seq_length x vocab_size\n",
        "        self.model.reset_states()\n",
        "        return self.g_predictions\n",
        "  def train_step(self, samples, rewards):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.get_loss(samples, rewards)\n",
        "\n",
        "        g_grad, _ = tf.clip_by_global_norm(\n",
        "            tape.gradient(loss, self.model.trainable_variables), 5.0)\n",
        "        g_updates = self.optimizer.apply_gradients(\n",
        "            zip(g_grad, self.model.trainable_variables))\n",
        "\n",
        "        return loss\n",
        "  def get_pretrain_loss(self, labels, samples): # labels as tokens, samples as prob distr\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, samples,from_logits=False)\n",
        "        return loss\n",
        "\n",
        "  def get_loss(self, x, rewards):\n",
        "      g_predictions = self.gen_predictions(x)\n",
        "      loss = -tf.reduce_sum(\n",
        "          tf.reduce_sum(\n",
        "              tf.one_hot(tf.cast(tf.reshape(x, [-1]), tf.int32), self.vocab_size,\n",
        "                          1.0, 0.0) * tf.math.log(\n",
        "                  tf.clip_by_value(\n",
        "                      tf.reshape(g_predictions, [-1, self.vocab_size]),\n",
        "                      1e-20, 1.0)\n",
        "              ), 1) * tf.reshape(rewards, [-1])\n",
        "      )\n",
        "      return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1tPh3d4ot4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}