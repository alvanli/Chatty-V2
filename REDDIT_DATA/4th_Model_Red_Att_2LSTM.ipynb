{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4th_Model_Red_Att_2LSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWhkRiycLwBQ",
        "colab_type": "text"
      },
      "source": [
        "This model is similar to the 3rd in that it both uses LSTM, this one, however, has one extra layer. It takes almost double the time to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soLYqDlVUh9f",
        "colab_type": "code",
        "outputId": "4a97943b-c14d-41b7-a2e8-b4577e2d4ff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "%cd ..\n",
        "%cd root\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"AL_clean_2.csv\")\n",
        "df.head()\n",
        "df.fillna('', inplace=True)\n",
        "print(len(df))\n",
        "x = df[\"x1\"].tolist()\n",
        "y = df[\"y1\"].tolist()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/root\n",
            "56297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF1aQLOeUlQO",
        "colab_type": "code",
        "outputId": "04dcacb1-e1ae-419c-c2f4-fabc3c346512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_words = set()\n",
        "target_words = set()\n",
        "\n",
        "for line in x:\n",
        "    input_texts.append(['\\t']+ str(line).split(\" \")+ ['\\n'])\n",
        "    for word in str(line).split(\" \"):\n",
        "        if word not in input_words:\n",
        "            input_words.add(word)\n",
        "\n",
        "for line in y:\n",
        "  target_texts.append(['\\t'] + str(line).split(\" \") + ['\\n'])\n",
        "  for word in str(line).split(\" \"):\n",
        "    if word not in target_words:\n",
        "        target_words.add(word)\n",
        "target_words.add(\"\\t\")\n",
        "target_words.add(\"\\n\")\n",
        "input_words.add(\"\\t\")\n",
        "input_words.add(\"\\n\")\n",
        "\n",
        "input_words = sorted(list(input_words))\n",
        "target_words = sorted(list(target_words))\n",
        "num_encoder_tokens = len(input_words)\n",
        "num_decoder_tokens = len(target_words)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 56297\n",
            "Number of unique input tokens: 7625\n",
            "Number of unique output tokens: 7292\n",
            "Max sequence length for inputs: 27\n",
            "Max sequence length for outputs: 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTXcGZveUp_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(target_words)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnbGWpTVUscc",
        "colab_type": "code",
        "outputId": "b39c034a-e798-4161-ea8e-3001c3a8c8c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length),\n",
        "    dtype='float32')\n",
        "output_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, word in enumerate(input_text):\n",
        "        input_data[i, t] = input_token_index[word]\n",
        "    for t, word in enumerate(target_text):\n",
        "        output_data[i, t] = target_token_index[word]\n",
        "\n",
        "# maybe expand dims?\n",
        "# import numpy as np\n",
        "# input_data = np.expand_dims(input_data, axis=-1)\n",
        "# output_data = np.expand_dims(output_data, axis=-1)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_data, output_data)).shuffle(len(input_data)).batch(64, drop_remainder=True)\n",
        "print(input_data.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(56297, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EViR_99Zb1v4",
        "colab_type": "code",
        "outputId": "70bf6a61-cbab-4bd6-c645-96155d1bf2b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 27]), TensorShape([64, 27]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch_AIZNQLozD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras import backend as K\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm1 = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                   return_sequences=True)\n",
        "    self.lstm2 = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    x = self.lstm1(x)\n",
        "    output, stateh, statec = self.lstm2(x, initial_state = hidden)\n",
        "    state = [stateh, statec]\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)),tf.zeros((self.batch_sz, self.enc_units))]\n",
        "\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "  def call(self, query, values):\n",
        "    query = K.sum(query, axis=0)  \n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm1 = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                   return_sequences=True)\n",
        "    self.lstm2 = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    # passing the concatenated vector to the GRU\n",
        "    x = self.lstm1(x)\n",
        "    output, stateh, statec = self.lstm2(x)\n",
        "    state = [stateh, statec]\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74HrJ7dtPlQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(num_encoder_tokens, 100, 1024, 64)\n",
        "decoder = Decoder(num_decoder_tokens, 100, 1024, 64)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQAknLonbjfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_token_index['\\t']] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krFa1QPYbuBf",
        "colab_type": "code",
        "outputId": "2810645c-ab68-4e73-8a14-16b8cbdabbaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "\n",
        "import time\n",
        "steps_per_epoch = len(input_data)//64\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.7126\n",
            "Epoch 1 Batch 100 Loss 1.9755\n",
            "Epoch 1 Batch 200 Loss 2.1146\n",
            "Epoch 1 Batch 300 Loss 1.8773\n",
            "Epoch 1 Batch 400 Loss 1.7733\n",
            "Epoch 1 Batch 500 Loss 1.9891\n",
            "Epoch 1 Batch 600 Loss 2.0662\n",
            "Epoch 1 Batch 700 Loss 1.6666\n",
            "Epoch 1 Batch 800 Loss 1.6719\n",
            "Epoch 1 Loss 1.8849\n",
            "Time taken for 1 epoch 436.0906000137329 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.5848\n",
            "Epoch 2 Batch 100 Loss 1.8971\n",
            "Epoch 2 Batch 200 Loss 1.7636\n",
            "Epoch 2 Batch 300 Loss 1.6071\n",
            "Epoch 2 Batch 400 Loss 1.5203\n",
            "Epoch 2 Batch 500 Loss 1.6501\n",
            "Epoch 2 Batch 600 Loss 1.7125\n",
            "Epoch 2 Batch 700 Loss 1.8210\n",
            "Epoch 2 Batch 800 Loss 1.8154\n",
            "Epoch 2 Loss 1.6971\n",
            "Time taken for 1 epoch 431.2004041671753 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7852\n",
            "Epoch 3 Batch 100 Loss 1.6061\n",
            "Epoch 3 Batch 200 Loss 1.6281\n",
            "Epoch 3 Batch 300 Loss 1.7245\n",
            "Epoch 3 Batch 400 Loss 1.6364\n",
            "Epoch 3 Batch 500 Loss 1.6458\n",
            "Epoch 3 Batch 600 Loss 1.6286\n",
            "Epoch 3 Batch 700 Loss 1.5751\n",
            "Epoch 3 Batch 800 Loss 1.6481\n",
            "Epoch 3 Loss 1.6223\n",
            "Time taken for 1 epoch 427.86418175697327 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.8611\n",
            "Epoch 4 Batch 100 Loss 1.5770\n",
            "Epoch 4 Batch 200 Loss 1.7191\n",
            "Epoch 4 Batch 300 Loss 1.4569\n",
            "Epoch 4 Batch 400 Loss 1.7940\n",
            "Epoch 4 Batch 500 Loss 1.4859\n",
            "Epoch 4 Batch 600 Loss 1.5428\n",
            "Epoch 4 Batch 700 Loss 1.3647\n",
            "Epoch 4 Batch 800 Loss 1.4852\n",
            "Epoch 4 Loss 1.5713\n",
            "Time taken for 1 epoch 429.4139664173126 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4412\n",
            "Epoch 5 Batch 100 Loss 1.6083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jknGyewrVpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VERez3Y3cA80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_decoder_seq_length, max_encoder_seq_length))\n",
        "\n",
        "  inputs = [input_token_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_encoder_seq_length,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  result = ''\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([target_token_index['\\t']], 0)\n",
        "  for t in range(max_decoder_seq_length):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += reverse_target_char_index[predicted_id] + ' '\n",
        "\n",
        "    if reverse_target_char_index[predicted_id] == '\\n':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7cYVYSOcF4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-si-rJFrkSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate(\"i want to die\")\n",
        "translate(\"can you kill me\")\n",
        "translate(\"i am sad\")\n",
        "translate(\"hello what is up\")\n",
        "translate(\"i made something\")\n",
        "translate(\"this is why you should not do this\")\n",
        "translate(\"can you stop\")\n",
        "translate(\"why do dogs have ears\")\n",
        "translate(\"why is trump nice\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}