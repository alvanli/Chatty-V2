{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mod3f.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fc47bf9d23894c8985d37096f524ccc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cb9089cd2af144b498c5c0dedf94f67f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6ba8b38e12f14e65afb7aa1183cbe4aa",
              "IPY_MODEL_2706acd1987944b89acb2ec3e6b314b3"
            ]
          }
        },
        "cb9089cd2af144b498c5c0dedf94f67f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ba8b38e12f14e65afb7aa1183cbe4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_003f79879cc04c81ae821c1ebe070bf1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 20,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b48b319f4dc4f7194c687f16a05642f"
          }
        },
        "2706acd1987944b89acb2ec3e6b314b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_76360830f06d425c934731266a23ef63",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20/20 [38:19&lt;00:00, 114.98s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1ca716b89894a15bc727bed062db2ae"
          }
        },
        "003f79879cc04c81ae821c1ebe070bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b48b319f4dc4f7194c687f16a05642f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76360830f06d425c934731266a23ef63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1ca716b89894a15bc727bed062db2ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tWhkRiycLwBQ"
      },
      "source": [
        "This model will try out attention with the original keras seq2seq model without embedding. try transformer maybe graph2seq, try reversing the training sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "soLYqDlVUh9f",
        "outputId": "5a6beb00-ab0d-4251-b28a-977124d5cba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "% cd .. \n",
        "% cd root\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"AL_clean_2.csv\")\n",
        "df.head()\n",
        "df.fillna('', inplace=True)\n",
        "print(len(df))\n",
        "x = df[\"x1\"].tolist()\n",
        "y = df[\"y1\"].tolist()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/root\n",
            "56820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VF1aQLOeUlQO",
        "outputId": "07ba7f27-5bd9-4139-942b-2a716c2c3d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_words = set()\n",
        "target_words = set()\n",
        "\n",
        "for line in x:\n",
        "    input_texts.append(['\\t']+ str(line).split(\" \")+ ['\\n'])\n",
        "    for word in str(line).split(\" \"):\n",
        "        if word not in input_words:\n",
        "            input_words.add(word)\n",
        "\n",
        "for line in y:\n",
        "  target_texts.append(['\\t'] + str(line).split(\" \") + ['\\n'])\n",
        "  for word in str(line).split(\" \"):\n",
        "    if word not in target_words:\n",
        "        target_words.add(word)\n",
        "target_words.add(\"\\t\")\n",
        "target_words.add(\"\\n\")\n",
        "input_words.add(\"\\t\")\n",
        "input_words.add(\"\\n\")\n",
        "\n",
        "input_words = sorted(list(input_words))\n",
        "target_words = sorted(list(target_words))\n",
        "num_encoder_tokens = len(input_words)\n",
        "num_decoder_tokens = len(target_words)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 56820\n",
            "Number of unique input tokens: 7747\n",
            "Number of unique output tokens: 7652\n",
            "Max sequence length for inputs: 27\n",
            "Max sequence length for outputs: 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OTXcGZveUp_-",
        "colab": {}
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(target_words)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MnbGWpTVUscc",
        "outputId": "37308f1c-1c8e-4c32-de7d-d27885c53fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length),\n",
        "    dtype='float32')\n",
        "output_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, word in enumerate(input_text):\n",
        "        input_data[i, t] = input_token_index[word]\n",
        "    for t, word in enumerate(target_text):\n",
        "        output_data[i, t] = target_token_index[word]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_data, output_data)).shuffle(len(input_data)).batch(64, drop_remainder=True)\n",
        "print(input_data.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(56820, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EViR_99Zb1v4",
        "outputId": "dd7417ec-ffc5-49ef-bf9c-5ede25ba7a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 27]), TensorShape([64, 27]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ch_AIZNQLozD",
        "colab": {}
      },
      "source": [
        "#https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.lstm(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units))]\n",
        "\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.lstm(x)\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "74HrJ7dtPlQY",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(num_encoder_tokens, 100, 1024, 64)\n",
        "decoder = Decoder(num_decoder_tokens, 100, 1024, 64)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LQAknLonbjfX",
        "colab": {}
      },
      "source": [
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_token_index['\\t']] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "krFa1QPYbuBf",
        "outputId": "d65cba64-9150-4cb8-fe86-49590ddc105a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "\n",
        "import time\n",
        "steps_per_epoch = len(input_data)//64\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.9136\n",
            "Epoch 1 Batch 100 Loss 2.0176\n",
            "Epoch 1 Batch 200 Loss 1.9658\n",
            "Epoch 1 Batch 300 Loss 1.6862\n",
            "Epoch 1 Batch 400 Loss 1.8112\n",
            "Epoch 1 Batch 500 Loss 1.9156\n",
            "Epoch 1 Batch 600 Loss 1.8646\n",
            "Epoch 1 Batch 700 Loss 1.6860\n",
            "Epoch 1 Batch 800 Loss 1.6355\n",
            "Epoch 1 Loss 1.8085\n",
            "Time taken for 1 epoch 296.54524874687195 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.8000\n",
            "Epoch 2 Batch 100 Loss 1.3788\n",
            "Epoch 2 Batch 200 Loss 1.5155\n",
            "Epoch 2 Batch 300 Loss 1.7700\n",
            "Epoch 2 Batch 400 Loss 1.5084\n",
            "Epoch 2 Batch 500 Loss 1.7422\n",
            "Epoch 2 Batch 600 Loss 1.5191\n",
            "Epoch 2 Batch 700 Loss 1.5995\n",
            "Epoch 2 Batch 800 Loss 1.6649\n",
            "Epoch 2 Loss 1.6327\n",
            "Time taken for 1 epoch 291.14478182792664 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.5845\n",
            "Epoch 3 Batch 100 Loss 1.7971\n",
            "Epoch 3 Batch 200 Loss 1.5024\n",
            "Epoch 3 Batch 300 Loss 1.3814\n",
            "Epoch 3 Batch 400 Loss 1.7189\n",
            "Epoch 3 Batch 500 Loss 1.4197\n",
            "Epoch 3 Batch 600 Loss 1.3501\n",
            "Epoch 3 Batch 700 Loss 1.4476\n",
            "Epoch 3 Batch 800 Loss 1.6562\n",
            "Epoch 3 Loss 1.5566\n",
            "Time taken for 1 epoch 295.0191881656647 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5035\n",
            "Epoch 4 Batch 100 Loss 1.3929\n",
            "Epoch 4 Batch 200 Loss 1.3368\n",
            "Epoch 4 Batch 300 Loss 1.3726\n",
            "Epoch 4 Batch 400 Loss 1.5974\n",
            "Epoch 4 Batch 500 Loss 1.4730\n",
            "Epoch 4 Batch 600 Loss 1.3146\n",
            "Epoch 4 Batch 700 Loss 1.2483\n",
            "Epoch 4 Batch 800 Loss 1.3853\n",
            "Epoch 4 Loss 1.4929\n",
            "Time taken for 1 epoch 293.38558745384216 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.3545\n",
            "Epoch 5 Batch 100 Loss 1.3920\n",
            "Epoch 5 Batch 200 Loss 1.4004\n",
            "Epoch 5 Batch 300 Loss 1.4402\n",
            "Epoch 5 Batch 400 Loss 1.4390\n",
            "Epoch 5 Batch 500 Loss 1.4947\n",
            "Epoch 5 Batch 600 Loss 1.4986\n",
            "Epoch 5 Batch 700 Loss 1.3915\n",
            "Epoch 5 Batch 800 Loss 1.5806\n",
            "Epoch 5 Loss 1.4271\n",
            "Time taken for 1 epoch 289.8856201171875 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.4239\n",
            "Epoch 6 Batch 100 Loss 1.4191\n",
            "Epoch 6 Batch 200 Loss 1.3892\n",
            "Epoch 6 Batch 300 Loss 1.5496\n",
            "Epoch 6 Batch 400 Loss 1.4277\n",
            "Epoch 6 Batch 500 Loss 1.2540\n",
            "Epoch 6 Batch 600 Loss 1.3046\n",
            "Epoch 6 Batch 700 Loss 1.3127\n",
            "Epoch 6 Batch 800 Loss 1.2347\n",
            "Epoch 6 Loss 1.3614\n",
            "Time taken for 1 epoch 288.5895268917084 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3874\n",
            "Epoch 7 Batch 100 Loss 1.2424\n",
            "Epoch 7 Batch 200 Loss 1.4012\n",
            "Epoch 7 Batch 300 Loss 1.2454\n",
            "Epoch 7 Batch 400 Loss 1.3450\n",
            "Epoch 7 Batch 500 Loss 1.3369\n",
            "Epoch 7 Batch 600 Loss 1.2958\n",
            "Epoch 7 Batch 700 Loss 1.2509\n",
            "Epoch 7 Batch 800 Loss 1.3769\n",
            "Epoch 7 Loss 1.2920\n",
            "Time taken for 1 epoch 294.3613440990448 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.0140\n",
            "Epoch 8 Batch 100 Loss 1.2084\n",
            "Epoch 8 Batch 200 Loss 1.2627\n",
            "Epoch 8 Batch 300 Loss 1.1905\n",
            "Epoch 8 Batch 400 Loss 1.3529\n",
            "Epoch 8 Batch 500 Loss 1.0516\n",
            "Epoch 8 Batch 600 Loss 1.1261\n",
            "Epoch 8 Batch 700 Loss 1.0364\n",
            "Epoch 8 Batch 800 Loss 1.2113\n",
            "Epoch 8 Loss 1.2294\n",
            "Time taken for 1 epoch 296.69239044189453 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.9643\n",
            "Epoch 9 Batch 100 Loss 1.2408\n",
            "Epoch 9 Batch 200 Loss 1.1381\n",
            "Epoch 9 Batch 300 Loss 0.9583\n",
            "Epoch 9 Batch 400 Loss 1.2897\n",
            "Epoch 9 Batch 500 Loss 1.0832\n",
            "Epoch 9 Batch 600 Loss 1.1737\n",
            "Epoch 9 Batch 700 Loss 1.1606\n",
            "Epoch 9 Batch 800 Loss 1.2778\n",
            "Epoch 9 Loss 1.1570\n",
            "Time taken for 1 epoch 295.1718430519104 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.0028\n",
            "Epoch 10 Batch 100 Loss 0.8958\n",
            "Epoch 10 Batch 200 Loss 0.9443\n",
            "Epoch 10 Batch 300 Loss 1.0224\n",
            "Epoch 10 Batch 400 Loss 1.0446\n",
            "Epoch 10 Batch 500 Loss 1.0621\n",
            "Epoch 10 Batch 600 Loss 1.0096\n",
            "Epoch 10 Batch 700 Loss 1.0473\n",
            "Epoch 10 Batch 800 Loss 1.0232\n",
            "Epoch 10 Loss 1.0850\n",
            "Time taken for 1 epoch 294.58491039276123 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4jknGyewrVpx",
        "colab": {}
      },
      "source": [
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPgvLF12kRVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder.save_weights(\"encoder.h5\")\n",
        "decoder.save_weights(\"decoder.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VERez3Y3cA80",
        "colab": {}
      },
      "source": [
        "#https://gist.github.com/udibr/67be473cf053d8c38730\n",
        "#https://github.com/heenabansal/seq2seq_chatbot_encoder_decoder_model_with_attention/blob/master/encoder_decoder_with_attention_model.py\n",
        "from tqdm.notebook import tqdm\n",
        "import math\n",
        "def evaluate(sentence):\n",
        "  inputs = [input_token_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_encoder_seq_length,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  result = ''\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "  sequences = [[[target_token_index['\\t']], 1, enc_hidden]]\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([target_token_index['\\t']], 0)\n",
        "  k1 = 3\n",
        "\n",
        "  for t in range(20):\n",
        "    all_candidates = []\n",
        "    ended_early = []\n",
        "    # print(\"Len Seq: {}\".format(len(sequences)))\n",
        "    for j in range(len(sequences)):\n",
        "      seq, score, dec_hidden = sequences[j]\n",
        "      last_word = seq[-1]\n",
        "      dec_input = tf.expand_dims([last_word], 0)\n",
        "      output, dec_hidden, _ = decoder(dec_input,dec_hidden,enc_out)\n",
        "      output = output[0]\n",
        "      for k in range(len(output)):\n",
        "        logged = -np.log(output[k])\n",
        "        logged = 1000 if math.isnan(logged) else logged\n",
        "        candidate = [seq+[k], score+logged, dec_hidden]\n",
        "        if k != 0:\n",
        "          all_candidates.append(candidate)\n",
        "        elif logged < 0.1:\n",
        "          ended_early.append(candidate)\n",
        "    ordered = sorted(all_candidates+ended_early, key=lambda tup:tup[1])\n",
        "    sequences = ordered[:3]\n",
        "  decoded_sentence, score, _ = sequences[0]\n",
        "  # decoded_sentence = decoded_sentence[0]\n",
        "  decoded_sentence = [reverse_target_char_index[x] for x in decoded_sentence if(x!=0)]\n",
        "  decoded_sentence = \" \".join(decoded_sentence)\n",
        "  return decoded_sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBy6qdA4tQu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing = pd.read_csv(\"testing.csv\", header=None)\n",
        "tests = testing[0].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l7cYVYSOcF4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fc47bf9d23894c8985d37096f524ccc0",
            "cb9089cd2af144b498c5c0dedf94f67f",
            "6ba8b38e12f14e65afb7aa1183cbe4aa",
            "2706acd1987944b89acb2ec3e6b314b3",
            "003f79879cc04c81ae821c1ebe070bf1",
            "6b48b319f4dc4f7194c687f16a05642f",
            "76360830f06d425c934731266a23ef63",
            "d1ca716b89894a15bc727bed062db2ae"
          ]
        },
        "outputId": "8d4431de-0811-437c-b27e-c77439ac5dd9"
      },
      "source": [
        "def translate(sentence):\n",
        "  samples = evaluate(sentence)\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(samples))\n",
        "for test in tqdm(tests):\n",
        "  translate(test)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc47bf9d23894c8985d37096f524ccc0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input: i am an amazon warrior it means i shop on amazon a lot\n",
            "Predicted translation: \t i could be your favorite reason i could be your username \n",
            " i am i am i have you get\n",
            "Input: how are you doing today\n",
            "Predicted translation: \t i have been waiting we will i have been waiting we will i have been waiting we will i am\n",
            "Input: i do not like you because of your face\n",
            "Predicted translation: \t i am i have a question i had a question i had a question i had a question i am\n",
            "Input: what do you think about trump\n",
            "Predicted translation: \t ff apps wouldnt watch spoon apps wouldnt watch spoon apps wouldnt watch spoon apps wouldnt watch spoon apps wouldnt eat\n",
            "Input: i suppose you are smart can you make me a sandwich\n",
            "Predicted translation: \t i am here \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Input: can you do math on your own\n",
            "Predicted translation: \t i would have a guilty mix \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Input: so i got a girlfriend today\n",
            "Predicted translation: \t i hope you we are you \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Input: what kind of pizza do you like\n",
            "Predicted translation: \t i adore orders to speak spoon apps wouldnt eat messing pee ibm dance dance spoon war legend combat learning russian\n",
            "Input: i am working on a website project \n",
            "Predicted translation: \t i could be your favorite \n",
            " i am i am i have you get a question i am i am\n",
            "Input: do you like chem homework\n",
            "Predicted translation: \t i would have a lot of it is it will i would have a lot of it was it was\n",
            "Input: i ate leftover pizza from yesterday it taste bad\n",
            "Predicted translation: \t i had a question i have you are there i have you are there i have you are there \n",
            "\n",
            "Input: is the sky purple when i am not looking\n",
            "Predicted translation: \t i am \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Input: hi my name is tommy and i am small\n",
            "Predicted translation: \t i am \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Input: would you rather dye your hair or eat a pear\n",
            "Predicted translation: \t i would be truth \n",
            " i would be truth \n",
            " i would be truth \n",
            " i would be truth \n",
            "\n",
            "Input: how many days does it take to build a bear\n",
            "Predicted translation: \t i am learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning\n",
            "Input: talking to you is difficult in so many ways\n",
            "Predicted translation: \t i am \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Input: congratulations i finished my homework\n",
            "Predicted translation: \t i am proud of luck we will i am so we will i am so we will i was a\n",
            "Input: can you guess why i like duck eggs when boiled with diamonds\n",
            "Predicted translation: \t i would have a guilty c c c c c c c c c c c c c c \n",
            "\n",
            "Input: you should not take pictures of cups in the morning\n",
            "Predicted translation: \t i could it was it was it was it was it was it was it was it was it was\n",
            "Input: my cousin is a musician and he likes french chocolate\n",
            "Predicted translation: \t i could verify i could have a handstand \n",
            " i was your wife \n",
            " i could have a handstand \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HDWpi27OFmA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "outputId": "594917fa-32dd-465c-d313-4558bc8b9dbb"
      },
      "source": [
        "\n",
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_decoder_seq_length, max_encoder_seq_length))\n",
        "\n",
        "  inputs = [input_token_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_encoder_seq_length,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  result = ''\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([target_token_index['\\t']], 0)\n",
        "  for t in range(max_decoder_seq_length):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += reverse_target_char_index[predicted_id] + ' '\n",
        "\n",
        "    if reverse_target_char_index[predicted_id] == '\\n':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot\n",
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "for test in tests:\n",
        "  translate(test)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: i am an amazon warrior it means i shop on amazon a lot\n",
            "Predicted translation: i am i am i am i am i am i am i am i am i am i am i am i am i am i \n",
            "Input: how are you doing today\n",
            "Predicted translation: i am waiting i am waiting i am waiting i am waiting i am waiting i am waiting i am waiting i am waiting i am waiting \n",
            "Input: i do not like you because of your face\n",
            "Predicted translation: i am i have a question i am i have a question i am i have a question i am i have a question i am i \n",
            "Input: what do you think about trump\n",
            "Predicted translation: ff apps wouldnt eat spoon apps wouldnt eat spoon apps wouldnt eat spoon apps wouldnt eat spoon apps wouldnt eat spoon apps wouldnt eat spoon apps wouldnt \n",
            "Input: i suppose you are smart can you make me a sandwich\n",
            "Predicted translation: i am in \n",
            " \n",
            "Input: can you do math on your own\n",
            "Predicted translation: i am a lot of \n",
            " \n",
            "Input: so i got a girlfriend today\n",
            "Predicted translation: i am \n",
            " \n",
            "Input: what kind of pizza do you like\n",
            "Predicted translation: i adore orders to chase spoon apps wouldnt eat pickled souls pee dutch write definedly x hunter x hunter x hunter x hunter x hunter x hunter \n",
            "Input: i am working on a website project \n",
            "Predicted translation: i am i am i am i am i am i am i am i am i am i am i am i am i am i \n",
            "Input: do you like chem homework\n",
            "Predicted translation: i am i am i am i am i am i am i am i am i am i am i am i am i am i \n",
            "Input: i ate leftover pizza from yesterday it taste bad\n",
            "Predicted translation: i am i have a question \n",
            " \n",
            "Input: is the sky purple when i am not looking\n",
            "Predicted translation: i have \n",
            " \n",
            "Input: hi my name is tommy and i am small\n",
            "Predicted translation: i am \n",
            " \n",
            "Input: would you rather dye your hair or eat a pear\n",
            "Predicted translation: i could be truth i had a \n",
            " \n",
            "Input: how many days does it take to build a bear\n",
            "Predicted translation: i am learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning \n",
            "Input: talking to you is difficult in so many ways\n",
            "Predicted translation: i am \n",
            " \n",
            "Input: congratulations i finished my homework\n",
            "Predicted translation: i am i am so we will i am i am so we will i am i am so we will i am i am so we \n",
            "Input: can you guess why i like duck eggs when boiled with diamonds\n",
            "Predicted translation: i am a good thing is a good thing is a good thing is a good thing is a good thing is a good thing is a \n",
            "Input: you should not take pictures of cups in the morning\n",
            "Predicted translation: i was it was it was it was it was it was it was it was it was it was it was it was it was it \n",
            "Input: my cousin is a musician and he likes french chocolate\n",
            "Predicted translation: i am a joke \n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}