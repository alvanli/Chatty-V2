{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mod3f.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tWhkRiycLwBQ"
      },
      "source": [
        "This model will try out attention with the original keras seq2seq model without embedding. try transformer maybe graph2seq, try reversing the training sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "soLYqDlVUh9f",
        "outputId": "787aa1c2-7ce0-4ed1-8fdf-c6414a2f3736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "% cd .. \n",
        "% cd root\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"AL_clean_2.csv\")\n",
        "df.head()\n",
        "df.fillna('', inplace=True)\n",
        "print(len(df))\n",
        "x = df[\"x1\"].tolist()\n",
        "y = df[\"y1\"].tolist()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/root\n",
            "56820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VF1aQLOeUlQO",
        "outputId": "99002389-debb-4e8f-edca-6bbd2274d639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_words = set()\n",
        "target_words = set()\n",
        "\n",
        "for line in x:\n",
        "    input_texts.append(['\\t']+ str(line).split(\" \")+ ['\\n'])\n",
        "    for word in str(line).split(\" \"):\n",
        "        if word not in input_words:\n",
        "            input_words.add(word)\n",
        "\n",
        "for line in y:\n",
        "  target_texts.append(['\\t'] + str(line).split(\" \") + ['\\n'])\n",
        "  for word in str(line).split(\" \"):\n",
        "    if word not in target_words:\n",
        "        target_words.add(word)\n",
        "target_words.add(\"\\t\")\n",
        "target_words.add(\"\\n\")\n",
        "input_words.add(\"\\t\")\n",
        "input_words.add(\"\\n\")\n",
        "\n",
        "input_words = sorted(list(input_words))\n",
        "target_words = sorted(list(target_words))\n",
        "num_encoder_tokens = len(input_words)\n",
        "num_decoder_tokens = len(target_words)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 56820\n",
            "Number of unique input tokens: 7747\n",
            "Number of unique output tokens: 7652\n",
            "Max sequence length for inputs: 27\n",
            "Max sequence length for outputs: 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OTXcGZveUp_-",
        "colab": {}
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(target_words)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MnbGWpTVUscc",
        "outputId": "ec58b68b-6ac7-4916-eb6a-cb2f38a3d64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length),\n",
        "    dtype='float32')\n",
        "output_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, word in enumerate(input_text):\n",
        "        input_data[i, t] = input_token_index[word]\n",
        "    for t, word in enumerate(target_text):\n",
        "        output_data[i, t] = target_token_index[word]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_data, output_data)).shuffle(len(input_data)).batch(64, drop_remainder=True)\n",
        "print(input_data.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(56820, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EViR_99Zb1v4",
        "outputId": "15378d6a-3ef6-4b37-9459-561befdfa48f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 27]), TensorShape([64, 27]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ch_AIZNQLozD",
        "colab": {}
      },
      "source": [
        "#https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.lstm(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units))]\n",
        "\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.lstm(x)\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "74HrJ7dtPlQY",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(num_encoder_tokens, 100, 1024, 64)\n",
        "decoder = Decoder(num_decoder_tokens, 100, 1024, 64)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LQAknLonbjfX",
        "colab": {}
      },
      "source": [
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_token_index['\\t']] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "krFa1QPYbuBf",
        "outputId": "3546c160-eaa6-43d2-cc9e-c5fe18a9fb91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "\n",
        "import time\n",
        "steps_per_epoch = len(input_data)//64\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.0823\n",
            "Epoch 1 Batch 100 Loss 2.1455\n",
            "Epoch 1 Batch 200 Loss 1.9582\n",
            "Epoch 1 Batch 300 Loss 1.9772\n",
            "Epoch 1 Batch 400 Loss 1.6747\n",
            "Epoch 1 Batch 500 Loss 1.8365\n",
            "Epoch 1 Batch 600 Loss 1.9542\n",
            "Epoch 1 Batch 700 Loss 1.4218\n",
            "Epoch 1 Batch 800 Loss 1.7910\n",
            "Epoch 1 Loss 1.8016\n",
            "Time taken for 1 epoch 437.4736111164093 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.5018\n",
            "Epoch 2 Batch 100 Loss 1.6304\n",
            "Epoch 2 Batch 200 Loss 1.6619\n",
            "Epoch 2 Batch 300 Loss 1.6288\n",
            "Epoch 2 Batch 400 Loss 1.7507\n",
            "Epoch 2 Batch 500 Loss 1.6243\n",
            "Epoch 2 Batch 600 Loss 1.4438\n",
            "Epoch 2 Batch 700 Loss 1.7368\n",
            "Epoch 2 Batch 800 Loss 1.7608\n",
            "Epoch 2 Loss 1.6510\n",
            "Time taken for 1 epoch 436.1582703590393 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.5679\n",
            "Epoch 3 Batch 100 Loss 1.6585\n",
            "Epoch 3 Batch 200 Loss 1.7286\n",
            "Epoch 3 Batch 300 Loss 1.6167\n",
            "Epoch 3 Batch 400 Loss 1.3119\n",
            "Epoch 3 Batch 500 Loss 1.7765\n",
            "Epoch 3 Batch 600 Loss 1.7065\n",
            "Epoch 3 Batch 700 Loss 1.6396\n",
            "Epoch 3 Batch 800 Loss 1.7995\n",
            "Epoch 3 Loss 1.5929\n",
            "Time taken for 1 epoch 435.63697814941406 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6342\n",
            "Epoch 4 Batch 100 Loss 1.4063\n",
            "Epoch 4 Batch 200 Loss 1.4527\n",
            "Epoch 4 Batch 300 Loss 1.5804\n",
            "Epoch 4 Batch 400 Loss 1.5838\n",
            "Epoch 4 Batch 500 Loss 1.6621\n",
            "Epoch 4 Batch 600 Loss 1.4873\n",
            "Epoch 4 Batch 700 Loss 1.4588\n",
            "Epoch 4 Batch 800 Loss 1.4853\n",
            "Epoch 4 Loss 1.5458\n",
            "Time taken for 1 epoch 433.6829237937927 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.5231\n",
            "Epoch 5 Batch 100 Loss 1.5458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4jknGyewrVpx",
        "colab": {}
      },
      "source": [
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPgvLF12kRVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder.save_weights(\"encoder.h5\")\n",
        "decoder.save_weights(\"decoder.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VERez3Y3cA80",
        "colab": {}
      },
      "source": [
        "#https://gist.github.com/udibr/67be473cf053d8c38730\n",
        "def evaluate(sentence):\n",
        "  inputs = [input_token_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_encoder_seq_length,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  result = ''\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([target_token_index['\\t']], 0)\n",
        "\n",
        "  k = 3\n",
        "  done_num = 0\n",
        "  done_samples = []\n",
        "  done_scores = []\n",
        "  going_num = 1\n",
        "  going_samples = tf.expand_dims([target_token_index['\\t']], 0)\n",
        "  going_scores = np.zeros((num_decoder_tokens))\n",
        "\n",
        "  while going_num and done_num < k:\n",
        "    predictions, dec_hidden, _ = decoder(dec_input,dec_hidden,enc_out)\n",
        "    predictions = predictions[0]\n",
        "    candidate_scores = np.array(going_scores)[:,None] - np.log(predictions)\n",
        "    flat_scores = candidate_scores.flatten()\n",
        "    ranks = flat_scores.argsort()[:(k-done_num)]\n",
        "    going_scores = flat_scores[ranks]\n",
        "    print(predictions.shape[0])\n",
        "    going_samples = [going_samples[r//num_decoder_tokens]+[r%num_decoder_tokens] for r in ranks]\n",
        "    \n",
        "    almost_done = [s[-1] == input_token_index['\\n'] or len(s) >= 25 for s in going_samples]\n",
        "\n",
        "    done_samples += [s for s,z in zip(going_samples, almost_done) if z]\n",
        "    done_scores += [s for s,z in zip(going_scores, almost_done) if z]\n",
        "    done_num = len(done_samples)\n",
        "    print(done_num)\n",
        "    going_samples = [s for s,z in zip(going_samples, almost_done) if not z]\n",
        "    going_scores = [s for s,z in zip(going_scores, almost_done) if not z]\n",
        "    going_num = len(going_samples)\n",
        "    print(going_num)\n",
        "\n",
        "  return going_samples + done_samples, going_scores + done_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l7cYVYSOcF4d",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  samples, scores = evaluate(sentence)\n",
        "  print(np.array(samples).shape)\n",
        "  # print(scores[2])\n",
        "  print(np.array(scores).shape)\n",
        "  # print('Input: %s' % (sentence))\n",
        "  # print('Predicted translation: {}'.format(result))\n",
        "testing = pd.read_csv(\"testing.csv\", header=None)\n",
        "tests = testing[0].tolist()\n",
        "translate(tests[0])\n",
        "# for test in tests:\n",
        "  # translate(test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}