{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seqgan_anno.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvaR2ji6h2Oo",
        "colab_type": "text"
      },
      "source": [
        "I will be annotating the entire seqGAN implementation! from https://github.com/tyo-yo/SeqGAN/tree/master/SeqGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTx93aR9hvyD",
        "colab_type": "text"
      },
      "source": [
        "This is rather straight forward, just a generator for pretraining, this is for step two of the algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPSLxU09hufa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Lambda, Activation, Dropout, Concatenate\n",
        "from keras.layers import Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D\n",
        "from keras.layers import Activation\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "def GeneratorPretraining(V, E, H):\n",
        "    '''\n",
        "    Model for Generator pretraining. This model's weights should be shared with\n",
        "        Generator.\n",
        "    # Arguments:\n",
        "        V: int, Vocabrary size\n",
        "        E: int, Embedding size\n",
        "        H: int, LSTM hidden size\n",
        "    # Returns:\n",
        "        generator_pretraining: keras Model\n",
        "            input: word ids, shape = (B, T)\n",
        "            output: word probability, shape = (B, T, V)\n",
        "    '''\n",
        "    # in comment, B means batch size, T means lengths of time steps.\n",
        "    input = Input(shape=(None,), dtype='int32', name='Input') # (B, T)\n",
        "    out = Embedding(V, E, mask_zero=True, name='Embedding')(input) # (B, T, E)\n",
        "    out = LSTM(H, return_sequences=True, name='LSTM')(out)  # (B, T, H)\n",
        "    out = TimeDistributed(\n",
        "        Dense(V, activation='softmax', name='DenseSoftmax'),\n",
        "        name='TimeDenseSoftmax')(out)    # (B, T, V)\n",
        "    generator_pretraining = Model(input, out)\n",
        "    return generator_pretraining"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz65a5xrhlM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator():\n",
        "    'Create Generator, which generate a next word.'\n",
        "    def __init__(self, sess, B, V, E, H, lr=1e-3):\n",
        "        '''\n",
        "        # Arguments:\n",
        "            B: int, Batch size\n",
        "            V: int, Vocabrary size\n",
        "            E: int, Embedding size\n",
        "            H: int, LSTM hidden size\n",
        "        # Optional Arguments:\n",
        "            lr: float, learning rate, default is 0.001\n",
        "        '''\n",
        "        self.sess = sess\n",
        "        self.B = B\n",
        "        self.V = V\n",
        "        self.E = E\n",
        "        self.H = H\n",
        "        self.lr = lr\n",
        "        self._build_graph()\n",
        "        self.reset_rnn_state()\n",
        "\n",
        "    def _build_graph(self):\n",
        "        state_in = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        h_in = tf.placeholder(tf.float32, shape=(None, self.H))\n",
        "        c_in = tf.placeholder(tf.float32, shape=(None, self.H))\n",
        "        action = tf.placeholder(tf.float32, shape=(None, self.V)) # onehot (B, V)\n",
        "        reward  =tf.placeholder(tf.float32, shape=(None, ))\n",
        "\n",
        "        self.layers = []\n",
        "        \n",
        "        embedding = Embedding(self.V, self.E, mask_zero=True, name='Embedding')\n",
        "        out = embedding(state_in) # (B, 1, E)\n",
        "        self.layers.append(embedding)\n",
        "        \n",
        "        lstm = LSTM(self.H, return_state=True, name='LSTM')\n",
        "        out, next_h, next_c = lstm(out, initial_state=[h_in, c_in])  # (B, H)\n",
        "        self.layers.append(lstm)\n",
        "\n",
        "        dense = Dense(self.V, activation='softmax', name='DenseSoftmax')\n",
        "        prob = dense(out)    # (B, V)\n",
        "        self.layers.append(dense)\n",
        "\n",
        "        log_prob = tf.log(tf.reduce_mean(prob * action, axis=-1)) # (B, )\n",
        "        loss = - log_prob * reward\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
        "        minimize = optimizer.minimize(loss)\n",
        "\n",
        "        self.state_in = state_in\n",
        "        self.h_in = h_in\n",
        "        self.c_in = c_in\n",
        "        self.action = action\n",
        "        self.reward = reward\n",
        "        self.prob = prob\n",
        "        self.next_h = next_h\n",
        "        self.next_c = next_c\n",
        "        self.minimize = minimize\n",
        "        self.loss = loss\n",
        "\n",
        "        self.init_op = tf.global_variables_initializer()\n",
        "        self.sess.run(self.init_op)\n",
        "\n",
        "    def reset_rnn_state(self):\n",
        "        self.h = np.zeros([self.B, self.H]) # reset hidden \n",
        "        self.c = np.zeros([self.B, self.H]) # reset cell state\n",
        "\n",
        "    def set_rnn_state(self, h, c):\n",
        "        '''\n",
        "        # Arguments:\n",
        "            h: np.array, shape = (B,H)\n",
        "            c: np.array, shape = (B,H)\n",
        "        '''\n",
        "        self.h = h\n",
        "        self.c = c\n",
        "\n",
        "    def get_rnn_state(self):\n",
        "        return self.h, self.c # h and c are rather consistent with LSTM outcomes\n",
        "\n",
        "    def predict(self, state, stateful=True):\n",
        "        '''\n",
        "        Predict next action(word) probability\n",
        "        # Arguments:\n",
        "            state: np.array, previous word ids, shape = (B, 1)\n",
        "        # Optional Arguments:\n",
        "            stateful: bool, default is True\n",
        "                if True, update rnn_state(h, c) to Generator.h, Generator.c\n",
        "                    and return prob.\n",
        "                else, return prob, next_h, next_c without updating states.\n",
        "        # Returns:\n",
        "            prob: np.array, shape=(B, V)\n",
        "        '''\n",
        "        # state = state.reshape(-1, 1)\n",
        "        feed_dict = {\n",
        "            self.state_in : state, # using h_{t-1} and x to predict the next token, x is state in this case\n",
        "            self.h_in : self.h,\n",
        "            self.c_in : self.c}\n",
        "        prob, next_h, next_c = self.sess.run(\n",
        "            [self.prob, self.next_h, self.next_c], # self-explanatory\n",
        "            feed_dict)\n",
        "\n",
        "        if stateful: # next state is the initial state of the next\n",
        "            self.h = next_h \n",
        "            self.c = next_c\n",
        "            return prob\n",
        "        else:\n",
        "            return prob, next_h, next_c\n",
        "\n",
        "    def update(self, state, action, reward, h=None, c=None, stateful=True):\n",
        "        '''\n",
        "        Update weights by Policy Gradient.\n",
        "        # Arguments:\n",
        "            state: np.array, Environment state, shape = (B, 1) or (B, t)\n",
        "                if shape is (B, t), state[:, -1] will be used.\n",
        "            action: np.array, Agent action, shape = (B, )\n",
        "                In training, action will be converted to onehot vector.\n",
        "                (Onehot shape will be (B, V))\n",
        "            reward: np.array, reward by Environment, shape = (B, )\n",
        "        # Optional Arguments:\n",
        "            h: np.array, shape = (B, H), default is None.\n",
        "                if None, h will be Generator.h\n",
        "            c: np.array, shape = (B, H), default is None.\n",
        "                if None, c will be Generator.c\n",
        "            stateful: bool, default is True\n",
        "                if True, update rnn_state(h, c) to Generator.h, Generator.c\n",
        "                    and return loss.\n",
        "                else, return loss, next_h, next_c without updating states.\n",
        "        # Returns:\n",
        "            loss: np.array, shape = (B, )\n",
        "            next_h: (if stateful is True)\n",
        "            next_c: (if stateful is True)\n",
        "        '''\n",
        "        if h is None:\n",
        "            h = self.h\n",
        "        if c is None:\n",
        "            c = self.c\n",
        "        state = state[:, -1].reshape(-1, 1)\n",
        "        reward = reward.reshape(-1)\n",
        "        feed_dict = {\n",
        "            self.state_in : state,\n",
        "            self.h_in : h,\n",
        "            self.c_in : c,\n",
        "            self.action : to_categorical(action, self.V),\n",
        "            self.reward : reward}\n",
        "        _, loss, next_h, next_c = self.sess.run(\n",
        "            [self.minimize, self.loss, self.next_h, self.next_c],\n",
        "            feed_dict)\n",
        "\n",
        "        if stateful:\n",
        "            self.h = next_h\n",
        "            self.c = next_c\n",
        "            return loss\n",
        "        else:\n",
        "            return loss, next_h, next_c\n",
        "\n",
        "    def sampling_word(self, prob):\n",
        "        '''\n",
        "        # Arguments:\n",
        "            prob: numpy array, dtype=float, shape = (B, V),\n",
        "        # Returns:\n",
        "            action: numpy array, dtype=int, shape = (B, )\n",
        "        '''\n",
        "        action = np.zeros((self.B,), dtype=np.int32)\n",
        "        for i in range(self.B):\n",
        "            p = prob[i]\n",
        "            action[i] = np.random.choice(self.V, p=p)\n",
        "        return action\n",
        "\n",
        "    def sampling_sentence(self, T, BOS=1):\n",
        "        '''\n",
        "        # Arguments:\n",
        "            T: int, max time steps\n",
        "        # Optional Arguments:\n",
        "            BOS: int, id for Begin Of Sentence\n",
        "        # Returns:\n",
        "            actions: numpy array, dtype=int, shape = (B, T)\n",
        "        '''\n",
        "        self.reset_rnn_state()\n",
        "        action = np.zeros([self.B, 1], dtype=np.int32)\n",
        "        action[:, 0] = BOS\n",
        "        actions = action\n",
        "        for _ in range(T):\n",
        "            prob = self.predict(action)\n",
        "            action = self.sampling_word(prob).reshape(-1, 1)\n",
        "            actions = np.concatenate([actions, action], axis=-1)\n",
        "        # Remove BOS\n",
        "        actions = actions[:, 1:]\n",
        "        self.reset_rnn_state()\n",
        "        return actions\n",
        "\n",
        "    def generate_samples(self, T, g_data, num, output_file):\n",
        "        '''\n",
        "        Generate sample sentences to output file\n",
        "        # Arguments:\n",
        "            T: int, max time steps\n",
        "            g_data: SeqGAN.utils.GeneratorPretrainingGenerator\n",
        "            num: int, number of sentences\n",
        "            output_file: str, path\n",
        "        '''\n",
        "        sentences=[]\n",
        "        for _ in range(num // self.B + 1):\n",
        "            actions = self.sampling_sentence(T)\n",
        "            actions_list = actions.tolist()\n",
        "            for sentence_id in actions_list:\n",
        "                sentence = [g_data.id2word[action] for action in sentence_id]\n",
        "                sentences.append(sentence)\n",
        "        output_str = ''\n",
        "        for i in range(num):\n",
        "            output_str += ' '.join(sentences[i]) + '\\n'\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(output_str)\n",
        "\n",
        "    def save(self, path):\n",
        "        weights = []\n",
        "        for layer in self.layers:\n",
        "            w = layer.get_weights()\n",
        "            weights.append(w)\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(weights, f)\n",
        "\n",
        "    def load(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            weights = pickle.load(f)\n",
        "        for layer, w in zip(self.layers, weights):\n",
        "            layer.set_weights(w)\n",
        "\n",
        "def Discriminator(V, E, H=64, dropout=0.1):\n",
        "    '''\n",
        "    Disciriminator model.\n",
        "    # Arguments:\n",
        "        V: int, Vocabrary size\n",
        "        E: int, Embedding size\n",
        "        H: int, LSTM hidden size\n",
        "        dropout: float\n",
        "    # Returns:\n",
        "        discriminator: keras model\n",
        "            input: word ids, shape = (B, T)\n",
        "            output: probability of true data or not, shape = (B, 1)\n",
        "    '''\n",
        "    input = Input(shape=(None,), dtype='int32', name='Input')   # (B, T)\n",
        "    out = Embedding(V, E, mask_zero=True, name='Embedding')(input)  # (B, T, E)\n",
        "    out = LSTM(H)(out)\n",
        "    out = Highway(out, num_layers=1)\n",
        "    out = Dropout(dropout, name='Dropout')(out)\n",
        "    out = Dense(1, activation='sigmoid', name='FC')(out)\n",
        "\n",
        "    discriminator = Model(input, out)\n",
        "    return discriminator\n",
        "\n",
        "def DiscriminatorConv(V, E, filter_sizes, num_filters, dropout):\n",
        "    '''\n",
        "    Another Discriminator model, currently unused because keras don't support\n",
        "    masking for Conv1D and it does huge influence on training.\n",
        "    # Arguments:\n",
        "        V: int, Vocabrary size\n",
        "        E: int, Embedding size\n",
        "        filter_sizes: list of int, list of each Conv1D filter sizes\n",
        "        num_filters: list of int, list of each Conv1D num of filters\n",
        "        dropout: float\n",
        "    # Returns:\n",
        "        discriminator: keras model\n",
        "            input: word ids, shape = (B, T)\n",
        "            output: probability of true data or not, shape = (B, 1)\n",
        "    '''\n",
        "    input = Input(shape=(None,), dtype='int32', name='Input')   # (B, T)\n",
        "    out = Embedding(V, E, name='Embedding')(input)  # (B, T, E)\n",
        "    out = VariousConv1D(out, filter_sizes, num_filters)\n",
        "    out = Highway(out, num_layers=1)\n",
        "    out = Dropout(dropout, name='Dropout')(out)\n",
        "    out = Dense(1, activation='sigmoid', name='FC')(out)\n",
        "\n",
        "    discriminator = Model(input, out)\n",
        "    return discriminator\n",
        "\n",
        "def VariousConv1D(x, filter_sizes, num_filters, name_prefix=''):\n",
        "    '''\n",
        "    Layer wrapper function for various filter sizes Conv1Ds\n",
        "    # Arguments:\n",
        "        x: tensor, shape = (B, T, E)\n",
        "        filter_sizes: list of int, list of each Conv1D filter sizes\n",
        "        num_filters: list of int, list of each Conv1D num of filters\n",
        "        name_prefix: str, layer name prefix\n",
        "    # Returns:\n",
        "        out: tensor, shape = (B, sum(num_filters))\n",
        "    '''\n",
        "    conv_outputs = []\n",
        "    for filter_size, n_filter in zip(filter_sizes, num_filters):\n",
        "        conv_name = '{}VariousConv1D/Conv1D/filter_size_{}'.format(name_prefix, filter_size)\n",
        "        pooling_name = '{}VariousConv1D/MaxPooling/filter_size_{}'.format(name_prefix, filter_size)\n",
        "        conv_out = Conv1D(n_filter, filter_size, name=conv_name)(x)   # (B, time_steps, n_filter)\n",
        "        conv_out = GlobalMaxPooling1D(name=pooling_name)(conv_out) # (B, n_filter)\n",
        "        conv_outputs.append(conv_out)\n",
        "    concatenate_name = '{}VariousConv1D/Concatenate'.format(name_prefix)\n",
        "    out = Concatenate(name=concatenate_name)(conv_outputs)\n",
        "    return out\n",
        "\n",
        "def Highway(x, num_layers=1, activation='relu', name_prefix=''):\n",
        "    '''\n",
        "    Layer wrapper function for Highway network\n",
        "    # Arguments:\n",
        "        x: tensor, shape = (B, input_size)\n",
        "    # Optional Arguments:\n",
        "        num_layers: int, dafault is 1, the number of Highway network layers\n",
        "        activation: keras activation, default is 'relu'\n",
        "        name_prefix: str, default is '', layer name prefix\n",
        "    # Returns:\n",
        "        out: tensor, shape = (B, input_size)\n",
        "    '''\n",
        "    input_size = K.int_shape(x)[1]\n",
        "    for i in range(num_layers):\n",
        "        gate_ratio_name = '{}Highway/Gate_ratio_{}'.format(name_prefix, i)\n",
        "        fc_name = '{}Highway/FC_{}'.format(name_prefix, i)\n",
        "        gate_name = '{}Highway/Gate_{}'.format(name_prefix, i)\n",
        "\n",
        "        gate_ratio = Dense(input_size, activation='sigmoid', name=gate_ratio_name)(x)\n",
        "        fc = Dense(input_size, activation=activation, name=fc_name)(x)\n",
        "        x = Lambda(lambda args: args[0] * args[2] + args[1] * (1 - args[2]), name=gate_name)([fc, x, gate_ratio])\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}