{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mod6a.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBAwsewrp8W9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "% cd .. \n",
        "% cd root\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "df = pd.read_csv(\"AL_clean_2.csv\")\n",
        "df.head()\n",
        "df.fillna('', inplace=True)\n",
        "print(len(df))\n",
        "x = df[\"x1\"].tolist()\n",
        "y = df[\"y1\"].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q4-OW3Tp9a2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "all_words = set()\n",
        "\n",
        "for line in x:\n",
        "    input_texts.append(['\\t']+ str(line).split(\" \")+ ['\\n'])\n",
        "    for word in str(line).split(\" \"):\n",
        "        if word not in all_words:\n",
        "            all_words.add(word)\n",
        "for line in y:\n",
        "  target_texts.append(['\\t'] + str(line).split(\" \") + ['\\n'])\n",
        "  for word in str(line).split(\" \"):\n",
        "    if word not in all_words:\n",
        "        all_words.add(word)\n",
        "all_words.add(\"\\t\")\n",
        "all_words.add(\"\\n\")\n",
        "\n",
        "all_words = sorted(list(all_words))\n",
        "num_tokens = len(all_words)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique tokens:', num_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9-hzYD9qgpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_index = dict([(word, i) for i, word in enumerate(all_words)])\n",
        "input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length),\n",
        "    dtype='float32')\n",
        "output_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, word in enumerate(input_text):\n",
        "        input_data[i, t] = token_index[word]\n",
        "    for t, word in enumerate(target_text):\n",
        "        output_data[i, t] = token_index[word]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_data, output_data)).shuffle(len(input_data)).batch(64, drop_remainder=True)\n",
        "print(input_data.shape)\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n",
        "reverse_index = dict((i, word) for word, i in token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvYfmZuUq1JI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4994ovBWFjk",
        "colab_type": "code",
        "outputId": "4256e85c-9628-4361-ca37-11a1b6b5e10b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Bidirectional, Dropout, Concatenate\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "\n",
        "def negative_crossentropy(y_true, y_pred):\n",
        "    return ( - K.categorical_crossentropy(y_true, y_pred))\n",
        "def GANmodel(maxlen_input, latent_dim, word_embedding_size=100,dictionary_size=num_tokens):\n",
        "  input_context = Input(shape=(maxlen_input,), dtype='int32', name='input_context')\n",
        "  input_answer = Input(shape=(maxlen_input,), dtype='int32', name='input_answer')\n",
        "  LSTM_encoder_bot = LSTM(latent_dim, init= 'lecun_uniform', name = 'encoder_bot')\n",
        "  LSTM_decoder_bot = LSTM(latent_dim, init= 'lecun_uniform', name = 'decoder_bot')\n",
        "  LSTM_encoder_discriminator = LSTM(latent_dim, init= 'lecun_uniform', trainable=False, name = 'encoder_discriminator')\n",
        "  LSTM_decoder_discriminator = LSTM(latent_dim, init= 'lecun_uniform', trainable=False, name = 'decoder_discriminator')\n",
        "  Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, input_length=maxlen_input, trainable=True, name = 'shared')\n",
        "\n",
        "  word_embedding_context = Shared_Embedding(input_context)\n",
        "  context_embedding_bot = LSTM_encoder_bot(word_embedding_context)\n",
        "  word_embedding_answer = Shared_Embedding(input_answer)\n",
        "  answer_embedding_bot = LSTM_decoder_bot(word_embedding_answer)\n",
        "  context_embedding_discriminator = LSTM_encoder_discriminator(word_embedding_context)\n",
        "  answer_embedding_discriminator = LSTM_decoder_discriminator(word_embedding_answer)\n",
        "\n",
        "  merge_layer = Concatenate(axis=1)([context_embedding_bot, answer_embedding_bot])\n",
        "  out = Dense(int(dictionary_size/2), activation=\"relu\")(merge_layer)\n",
        "  out = Dense(int(dictionary_size), activation=\"softmax\")(out)\n",
        "  loss = Concatenate(axis=1)([context_embedding_discriminator, answer_embedding_discriminator, out])\n",
        "  loss = Dense(1, activation=\"sigmoid\", trainable=False, name = 'discriminator_output')(loss)\n",
        "\n",
        "  model = Model(input=[input_context, input_answer], output = [loss])\n",
        "  return model\n",
        "model = GANmodel(27, 128)\n",
        "model.compile(loss=negative_crossentropy, optimizer=\"adam\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, name=\"encoder_bot\", kernel_initializer=\"lecun_uniform\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, name=\"decoder_bot\", kernel_initializer=\"lecun_uniform\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, trainable=False, name=\"encoder_discriminator\", kernel_initializer=\"lecun_uniform\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, trainable=False, name=\"decoder_discriminator\", kernel_initializer=\"lecun_uniform\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}