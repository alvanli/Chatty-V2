{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1810.04805.pdf\n",
    "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\n",
    "https://colab.research.google.com/drive/1hMLd5-r82FrnFnBub-B-fVW78Px4KPX1#scrollTo=IW6V3afD-q1K\n",
    "https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what kind of phones do you guys have</td>\n",
       "      <td>i have a it is pretty great much better than w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have a it is pretty great much better than w...</td>\n",
       "      <td>does it really charge all the way in min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does it really charge all the way in min</td>\n",
       "      <td>pretty fast i have never it but it is under ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what kind of phones do you guys have</td>\n",
       "      <td>samsung galaxy j it is my first cell phone and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>samsung galaxy j it is my first cell phone and...</td>\n",
       "      <td>what do you think of it anything you do not like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  x1  \\\n",
       "0               what kind of phones do you guys have   \n",
       "1  i have a it is pretty great much better than w...   \n",
       "2           does it really charge all the way in min   \n",
       "3               what kind of phones do you guys have   \n",
       "4  samsung galaxy j it is my first cell phone and...   \n",
       "\n",
       "                                                  y1  \n",
       "0  i have a it is pretty great much better than w...  \n",
       "1           does it really charge all the way in min  \n",
       "2  pretty fast i have never it but it is under ha...  \n",
       "3  samsung galaxy j it is my first cell phone and...  \n",
       "4   what do you think of it anything you do not like  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read from dataset and split into input and targets\n",
    "df = pd.read_csv(\"char_cleaned_data3.csv\")\n",
    "df.dropna(subset = [\"x1\"], inplace=True)\n",
    "df.dropna(subset = [\"y1\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47365\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10000\n",
    "\n",
    "print(len(df))\n",
    "questions = df[\"x1\"].tolist()[:num_samples]\n",
    "answers = df[\"y1\"].tolist()[:num_samples]\n",
    "\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set()\n",
    "for line in questions[:10]:\n",
    "    t = tokenizer.tokenize(line)\n",
    "    for i in t:\n",
    "        tokens.add(i)\n",
    "    \n",
    "tokens.add(\"[CLS]\")\n",
    "tokens.add(\"[SEP]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##imum',\n",
       " '##s',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " 'a',\n",
       " 'all',\n",
       " 'and',\n",
       " 'anything',\n",
       " 'before',\n",
       " 'better',\n",
       " 'cell',\n",
       " 'charge',\n",
       " 'do',\n",
       " 'does',\n",
       " 'first',\n",
       " 'for',\n",
       " 'friend',\n",
       " 'galaxy',\n",
       " 'great',\n",
       " 'guys',\n",
       " 'had',\n",
       " 'have',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'j',\n",
       " 'kill',\n",
       " 'kind',\n",
       " 'know',\n",
       " 'like',\n",
       " 'me',\n",
       " 'min',\n",
       " 'months',\n",
       " 'much',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'not',\n",
       " 'of',\n",
       " 'old',\n",
       " 'op',\n",
       " 'opt',\n",
       " 'phone',\n",
       " 'phones',\n",
       " 'pretty',\n",
       " 'really',\n",
       " 'samsung',\n",
       " 'than',\n",
       " 'the',\n",
       " 'think',\n",
       " 'to',\n",
       " 'told',\n",
       " 'v',\n",
       " 'way',\n",
       " 'what',\n",
       " 'you',\n",
       " 'yourself'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'kind', 'of', 'phones', 'do', 'you', 'guys', 'have', '[SEP]']\n",
      "['[CLS]', 'i', 'have', 'a', 'it', 'is', 'pretty', 'great', 'much', 'better', 'than', 'what', 'i', 'had', 'before', '[SEP]']\n",
      "['[CLS]', 'does', 'it', 'really', 'charge', 'all', 'the', 'way', 'in', 'min', '[SEP]']\n",
      "['[CLS]', 'what', 'kind', 'of', 'phones', 'do', 'you', 'guys', 'have', '[SEP]']\n",
      "['[CLS]', 'samsung', 'galaxy', 'j', 'it', 'is', 'my', 'first', 'cell', 'phone', 'and', 'i', 'have', 'had', 'it', 'for', 'months', '[SEP]']\n",
      "['[CLS]', 'what', 'do', 'you', 'think', 'of', 'it', 'anything', 'you', 'do', 'not', 'like', '[SEP]']\n",
      "['[CLS]', 'what', 'kind', 'of', 'phones', 'do', 'you', 'guys', 'have', '[SEP]']\n",
      "['[CLS]', 'opt', '##imum', '##s', 'v', 'i', 'know', 'it', 'is', 'old', '[SEP]']\n",
      "['[CLS]', 'my', 'friend', 'told', 'me', 'to', 'kill', 'myself', '[SEP]']\n",
      "['[CLS]', 'do', 'not', 'kill', 'yourself', 'op', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "def tokenize (s):\n",
    "    stokens = tokenizer.tokenize(s)\n",
    "    stokens =  [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "    \n",
    "    return stokens\n",
    "\n",
    "input_ids = []\n",
    "input_masks = []\n",
    "input_segments = []\n",
    "\n",
    "for line in questions[:10]:\n",
    "    stokens = tokenize(line)\n",
    "    print(stokens)\n",
    "    input_ids.append(get_ids(stokens, tokenizer, max_seq_length))\n",
    "    input_masks.append(get_masks(stokens, max_seq_length))\n",
    "    input_segments.append(get_segments(stokens, max_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 2054, 2785, 1997, 11640, 2079, 2017, 4364, 2031, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2031, 1037, 2009, 2003, 3492, 2307, 2172, 2488, 2084, 2054, 1045, 2018, 2077, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2515, 2009, 2428, 3715, 2035, 1996, 2126, 1999, 8117, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2785, 1997, 11640, 2079, 2017, 4364, 2031, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 19102, 9088, 1046, 2009, 2003, 2026, 2034, 3526, 3042, 1998, 1045, 2031, 2018, 2009, 2005, 2706, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2079, 2017, 2228, 1997, 2009, 2505, 2017, 2079, 2025, 2066, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2785, 1997, 11640, 2079, 2017, 4364, 2031, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 23569, 28591, 2015, 1058, 1045, 2113, 2009, 2003, 2214, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2026, 2767, 2409, 2033, 2000, 3102, 2870, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2079, 2025, 3102, 4426, 6728, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = np.array(input_ids)\n",
    "input_masks= np.array(input_masks)\n",
    "input_segments = np.array(input_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "#     \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "#     if isinstance(example, PaddingInputExample):\n",
    "#         input_ids = [0] * max_seq_length\n",
    "#         input_mask = [0] * max_seq_length\n",
    "#         segment_ids = [0] * max_seq_length\n",
    "#         label = 0\n",
    "#         return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "#     tokens_a = tokenizer.tokenize(example)\n",
    "#     if len(tokens_a) > max_seq_length - 2:\n",
    "#         tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "#     tokens = []\n",
    "#     segment_ids = []\n",
    "#     tokens.append(\"[CLS]\")\n",
    "#     segment_ids.append(0)\n",
    "#     for token in tokens_a:\n",
    "#         tokens.append(token)\n",
    "#         segment_ids.append(0)\n",
    "#     tokens.append(\"[SEP]\")\n",
    "#     segment_ids.append(0)\n",
    "\n",
    "#     input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "#     # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "#     # tokens are attended to.\n",
    "#     input_mask = [1] * len(input_ids)\n",
    "\n",
    "#     # Zero-pad up to the sequence length.\n",
    "#     while len(input_ids) < max_seq_length:\n",
    "#         input_ids.append(0)\n",
    "#         input_mask.append(0)\n",
    "#         segment_ids.append(0)\n",
    "\n",
    "#     assert len(input_ids) == max_seq_length\n",
    "#     assert len(input_mask) == max_seq_length\n",
    "#     assert len(segment_ids) == max_seq_length\n",
    "\n",
    "#     return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "\n",
    "# def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "#     \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "#     input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "#     for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "#         input_id, input_mask, segment_id, label = convert_single_example(\n",
    "#             tokenizer, example, max_seq_length\n",
    "#         )\n",
    "#         input_ids.append(input_id)\n",
    "#         input_masks.append(input_mask)\n",
    "#         segment_ids.append(segment_id)\n",
    "#         labels.append(label)\n",
    "#     return (\n",
    "#         np.array(input_ids),\n",
    "#         np.array(input_masks),\n",
    "#         np.array(segment_ids),\n",
    "#         np.array(labels).reshape(-1, 1),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_embs, all_embs = model.predict([input_ids,input_masks,input_segments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3.67991626e-01,  2.62976646e-01, -9.17949900e-02, ...,\n",
       "         -3.43436956e-01,  1.00064501e-01,  3.28222066e-01],\n",
       "        [ 2.88614124e-01, -1.47708416e-01, -3.50446224e-01, ...,\n",
       "         -3.76085639e-01,  1.22872666e-01, -2.93080926e-01],\n",
       "        [ 9.05614257e-01, -3.38526458e-01,  1.01535487e+00, ...,\n",
       "         -3.64174902e-01, -7.62960389e-02, -9.65847194e-01],\n",
       "        ...,\n",
       "        [ 3.69523287e-01,  1.84878424e-01,  1.90430164e-01, ...,\n",
       "          2.34707654e-01,  1.58575922e-02,  1.34283043e-02],\n",
       "        [ 2.84774601e-01,  1.19270198e-01,  1.98776037e-01, ...,\n",
       "          2.09813222e-01, -7.27497786e-03, -3.44730616e-02],\n",
       "        [ 3.29375029e-01,  1.48037493e-01,  3.57326537e-01, ...,\n",
       "          1.69766128e-01, -2.93716788e-04, -1.96482390e-02]],\n",
       "\n",
       "       [[ 1.37833685e-01,  1.95774108e-01,  8.44631419e-02, ...,\n",
       "         -1.22579433e-01,  1.22901604e-01,  3.55702698e-01],\n",
       "        [ 3.76964927e-01,  3.50292265e-01,  3.04621458e-01, ...,\n",
       "         -1.58174992e-01,  9.36976075e-01,  1.78167135e-01],\n",
       "        [ 1.12278938e+00,  6.92525387e-01,  6.15613043e-01, ...,\n",
       "         -9.02498484e-01,  2.10210606e-02,  2.29451314e-01],\n",
       "        ...,\n",
       "        [-1.71818584e-01, -4.60843563e-01,  3.79261702e-01, ...,\n",
       "          1.09369330e-01, -1.71375901e-01,  5.08151501e-02],\n",
       "        [-8.09836388e-02, -3.55681568e-01,  3.89374435e-01, ...,\n",
       "         -2.14129016e-02, -2.32715324e-01, -1.85786206e-02],\n",
       "        [-2.68402964e-01, -4.55588162e-01,  4.42666501e-01, ...,\n",
       "          2.41155475e-01, -1.46414265e-02,  5.22758029e-02]],\n",
       "\n",
       "       [[-3.03112924e-01, -1.34366676e-01,  3.26916635e-01, ...,\n",
       "         -7.01411068e-02,  2.84147739e-01,  6.45859718e-01],\n",
       "        [-1.00903109e-01, -2.08484530e-01,  3.30045074e-01, ...,\n",
       "          3.88129443e-01,  5.88835955e-01,  2.45951086e-01],\n",
       "        [-1.24209382e-01, -7.20687091e-01,  5.16129017e-01, ...,\n",
       "         -3.11058372e-01,  1.22793324e-01, -5.74776754e-02],\n",
       "        ...,\n",
       "        [-7.10135251e-02,  1.00597948e-01,  3.79591554e-01, ...,\n",
       "          3.05843771e-01,  9.97526497e-02,  1.81953311e-01],\n",
       "        [-4.93079051e-02,  1.92429334e-01,  4.67548132e-01, ...,\n",
       "          2.79148608e-01,  5.42457849e-02,  1.93933293e-01],\n",
       "        [-8.85999054e-02,  8.90457407e-02,  4.25271124e-01, ...,\n",
       "          1.77453190e-01,  6.46324307e-02,  2.48477057e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.20658994e-01,  1.25900581e-01,  4.67376173e-01, ...,\n",
       "         -1.16204351e-01,  2.78332889e-01,  4.59985048e-01],\n",
       "        [-3.19189966e-01,  1.30146533e-01,  2.44377166e-01, ...,\n",
       "         -2.30756223e-01, -5.00701070e-02,  3.37146729e-01],\n",
       "        [-7.94778585e-01,  4.29505587e-01,  3.36805463e-01, ...,\n",
       "         -2.56396890e-01,  4.92054403e-01, -2.65206307e-01],\n",
       "        ...,\n",
       "        [ 6.71948791e-02, -6.52991980e-02,  9.95223880e-01, ...,\n",
       "          1.99559152e-01,  1.24525107e-01,  2.25088313e-01],\n",
       "        [ 8.19153637e-02,  6.94037378e-02,  4.94662941e-01, ...,\n",
       "         -7.35584274e-03,  4.84193154e-02, -1.01506487e-01],\n",
       "        [ 1.50339797e-01, -4.27550152e-02,  9.75829720e-01, ...,\n",
       "          1.25791848e-01,  9.99248922e-02,  1.19672723e-01]],\n",
       "\n",
       "       [[ 1.79004163e-01,  2.51837760e-01, -4.68794107e-02, ...,\n",
       "         -2.58457139e-02,  9.91029665e-02,  1.53563082e-01],\n",
       "        [ 1.90177172e-01, -3.25951785e-01, -6.19755685e-02, ...,\n",
       "         -1.55931279e-01,  1.63672403e-01,  6.83321178e-01],\n",
       "        [ 4.27648097e-01, -5.00853002e-01,  1.97428137e-01, ...,\n",
       "          1.23650700e-01, -5.65792143e-01,  3.65991563e-01],\n",
       "        ...,\n",
       "        [ 2.97908396e-01,  1.41846597e-01,  5.20776987e-01, ...,\n",
       "          3.00886333e-01, -9.71210301e-02,  2.85281479e-01],\n",
       "        [ 3.33669841e-01,  1.18026376e-01,  4.46985066e-01, ...,\n",
       "          3.43874186e-01, -9.19891298e-02,  2.20171958e-01],\n",
       "        [ 3.05835485e-01,  1.62351608e-01,  5.24762034e-01, ...,\n",
       "          3.42915714e-01, -1.30457088e-01,  1.47660464e-01]],\n",
       "\n",
       "       [[ 5.13469428e-03,  3.50693733e-01, -2.16590211e-01, ...,\n",
       "         -2.02858120e-01,  2.53281116e-01,  4.59423274e-01],\n",
       "        [ 3.95590216e-01,  1.45281553e-01, -6.63713098e-01, ...,\n",
       "          3.16400617e-01,  2.71335512e-01,  2.56729960e-01],\n",
       "        [ 2.16391757e-01, -6.89551830e-01, -2.24789962e-01, ...,\n",
       "          4.11336273e-02, -1.89469457e-01,  1.42498359e-01],\n",
       "        ...,\n",
       "        [ 2.79613316e-01,  4.73348588e-01,  4.12563503e-01, ...,\n",
       "          1.60403714e-01, -2.72549778e-01,  2.92145610e-01],\n",
       "        [ 2.33264327e-01,  5.23051381e-01,  3.54963392e-01, ...,\n",
       "          1.61126718e-01, -2.89693117e-01,  3.26092690e-01],\n",
       "        [ 3.39531541e-01,  4.96392757e-01,  5.22870302e-01, ...,\n",
       "          7.98566490e-02, -3.14131409e-01,  6.31998330e-02]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
